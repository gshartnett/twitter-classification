{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The goal of this notebook is build two models, Naive Bayes and Maximum Entropy, \n",
    "## and to compare their performances on various CV data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data set size: 891561\n",
      "training-cv data set size: 222891\n",
      "Sentiment 140 (S140) cv data set size: 497\n",
      "Sanders anlaytics (SA) cv data set size: 2624\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import scipy.sparse\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "## display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "## load the processed training-set tweets\n",
    "## use the small processed training-set tweets for debugging\n",
    "small = False\n",
    "if small:\n",
    "  df = pd.read_pickle('../tweet_data/training_data/training_data_small_processed.pkl')\n",
    "  df_cv = pd.read_pickle('../tweet_data/training_data/training_data_small_cv_processed.pkl') \n",
    "else:\n",
    "  df = pd.read_pickle('../tweet_data/training_data/training_data_processed.pkl')\n",
    "  df_cv = pd.read_pickle('../tweet_data/training_data/training_data_cv_processed.pkl')  \n",
    "\n",
    "## load the processed test-set tweets\n",
    "df_sa = pd.read_pickle('../tweet_data/cv_data/sanders_analytics/sanders_analytics_cv_data_processed.pkl')\n",
    "df_s140 = pd.read_pickle('../tweet_data/cv_data/sentiment140/sentiment140_cv_data_processed.pkl')\n",
    "\n",
    "if small:\n",
    "  path = '../model_parameters/small_'\n",
    "else:\n",
    "  path = '../model_parameters/'\n",
    "  \n",
    "print 'training data set size:', len(df)\n",
    "print 'training-cv data set size:', + len(df_cv)\n",
    "print 'Sentiment 140 (S140) cv data set size:', len(df_s140)\n",
    "print 'Sanders anlaytics (SA) cv data set size:', len(df_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training emoticon fractions: [0.06314206206866384, 0.06759380457422431]\n",
      "CV emoticon fractions: [0.061779075871165726, 0.06762049611693607]\n",
      "S140 emoticon fractions: [0.06779661016949153, 0.11049723756906077]\n",
      "SA emoticon fractions: [0.024282560706401765, 0.05089058524173028]\n"
     ]
    }
   ],
   "source": [
    "## some useful functions\n",
    "\n",
    "## sentiment statistics\n",
    "def sentiment_stats(DF):\n",
    "  return [1.0*sum(DF['sentiment']==-1)/len(DF), 1.0*sum(DF['sentiment']==0)/len(DF), \\\n",
    "          1.0*sum(DF['sentiment']==1)/len(DF)]\n",
    "\n",
    "## emoticon statistics\n",
    "def emoticon_stats(DF):\n",
    "  neg_emoticon_counter = 0\n",
    "  pos_emoticon_counter = 0\n",
    "  for i in range(len(DF)):\n",
    "    if DF['sentiment'].iloc[i] == -1:\n",
    "      neg_emoticon_counter += Counter(DF['words'].iloc[i])['negemoticontoken']\n",
    "    if DF['sentiment'].iloc[i] == 1:\n",
    "      pos_emoticon_counter += Counter(DF['words'].iloc[i])['posemoticontoken']\n",
    "  return [float(neg_emoticon_counter)/sum(DF['sentiment']==-1), \\\n",
    "          float(pos_emoticon_counter)/sum(DF['sentiment']==1)]\n",
    "\n",
    "print 'training emoticon fractions:', emoticon_stats(df)\n",
    "print 'CV emoticon fractions:', emoticon_stats(df_cv)\n",
    "print 'S140 emoticon fractions:', emoticon_stats(df_s140)\n",
    "print 'SA emoticon fractions:', emoticon_stats(df_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## build the bag of words\n",
    "ranks_tot = [item for sublist in df['word_ranks'] for item in sublist]\n",
    "ranks_tot_neg = [item for sublist in df['word_ranks'][df['sentiment'] == -1] for item in sublist]\n",
    "ranks_tot_neutral = [item for sublist in df['word_ranks'][df['sentiment'] == 0] for item in sublist]\n",
    "ranks_tot_pos = [item for sublist in df['word_ranks'][df['sentiment'] == 1] for item in sublist]\n",
    "counter = Counter(ranks_tot)\n",
    "counter_neg = Counter(ranks_tot_neg)\n",
    "counter_neutral = Counter(ranks_tot_neutral)\n",
    "counter_pos = Counter(ranks_tot_pos)\n",
    "max_rank = max(ranks_tot)+1\n",
    "\n",
    "## put the CV sets into word rank form\n",
    "words_tot = [item for sublist in df['words'] for item in sublist]\n",
    "count = Counter(words_tot).most_common()\n",
    "words_unique = [count[i][0] for i in range(len(count))]\n",
    "words_frequencies = [count[i][1] for i in range(len(count))]\n",
    "\n",
    "def word_to_rank(x):\n",
    "  return sorted([ words_unique.index(w) for w in x if w in words_unique])\n",
    "\n",
    "df_s140['word_ranks'] = df_s140['words'].map(word_to_rank)\n",
    "df_sa['word_ranks'] = df_sa['words'].map(word_to_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[u'wa', u'talking', u'guy', u'last', u'night', u'wa', u'telling', u'die', u'hard', u'spur', u'fan', u'also', u'told', u'hate', u'lebron', u'james']\n",
      "[5, 5, 78, 88, 92, 122, 128, 202, 256, 545, 551, 631, 1181, 1642, 6042, 12014]\n"
     ]
    }
   ],
   "source": [
    "## example for blog write-up\n",
    "print len(df_s140['words'].iloc[15]) - len(df_s140['word_ranks'].iloc[15])\n",
    "print df_s140['words'].iloc[15] \n",
    "print df_s140['word_ranks'].iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance:\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "| data set | accuracy | (-) accuracy | (0) accuracy | (+) accuracy |\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "|    CV    |   77.8   |     68.7     |    93.85     |    70.87     |\n",
      "|   S140   |  55.73   |    50.28     |    64.75     |    54.14     |\n",
      "|    SA    |  51.94   |    33.77     |    57.82     |    46.31     |\n",
      "+----------+----------+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "##re-think Laplace smoothing now that I'm ignoring new words in cv set\n",
    "\n",
    "## priors\n",
    "prior_neg = float(sum(df['sentiment']==-1))/len(df)\n",
    "prior_neutral = float(sum(df['sentiment']==0))/len(df)\n",
    "prior_pos = float(sum(df['sentiment']==1))/len(df)\n",
    "\n",
    "## conditional probabilities of word given class (with Laplace smoothing parameter alpha)\n",
    "alpha = 1 # Laplace smoothing parameter\n",
    "def p_rank_given_neg(rank):\n",
    "  return float(counter_neg[rank] + alpha)/(len(ranks_tot_neg) + alpha*len(ranks_tot))\n",
    "def p_rank_given_neutral(rank):\n",
    "  return float(counter_neutral[rank] + alpha)/(len(ranks_tot_neutral) + alpha*len(ranks_tot))\n",
    "def p_rank_given_pos(rank):\n",
    "  return float(counter_pos[rank] + alpha)/(len(ranks_tot_pos) + alpha*len(ranks_tot))\n",
    "\n",
    "## class prediction\n",
    "def NB_predict(rank_list):\n",
    "  p_neg = math.log(prior_neg) + sum([math.log(p_rank_given_neg(i)) for i in rank_list])\n",
    "  p_neutral = math.log(prior_neutral) + sum([math.log(p_rank_given_neutral(i)) for i in rank_list])\n",
    "  p_pos = math.log(prior_pos) + sum([math.log(p_rank_given_pos(i)) for i in rank_list])\n",
    "  return -1 + [p_neg, p_neutral, p_pos].index(max([p_neg, p_neutral, p_pos]))\n",
    "\n",
    "df_cv['NB predictions'] = df_cv['word_ranks'].map(NB_predict)\n",
    "df_sa['NB predictions'] = df_sa['word_ranks'].map(NB_predict)\n",
    "df_s140['NB predictions'] = df_s140['word_ranks'].map(NB_predict)\n",
    "\n",
    "## Classification error\n",
    "## training CV set\n",
    "NB_accuracy_cv = 100*float(sum(df_cv['NB predictions'] == df_cv['sentiment']))/len(df_cv)\n",
    "NB_accuracy_cv_neg = 100.0*sum(df_cv[df_cv['sentiment'] == -1]['NB predictions'] == df_cv[df_cv['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == -1])\n",
    "NB_accuracy_cv_neutral = 100.0*sum(df_cv[df_cv['sentiment'] == 0]['NB predictions'] == df_cv[df_cv['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == 0])\n",
    "NB_accuracy_cv_pos = 100.0*sum(df_cv[df_cv['sentiment'] == 1]['NB predictions'] == df_cv[df_cv['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == 1])\n",
    "## S140 set\n",
    "NB_accuracy_s140 = 100*float(sum(df_s140['NB predictions'] == df_s140['sentiment']))/len(df_s140)\n",
    "NB_accuracy_s140_neg = 100.0*sum(df_s140[df_s140['sentiment'] == -1]['NB predictions'] == df_s140[df_s140['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == -1])\n",
    "NB_accuracy_s140_neutral = 100.0*sum(df_s140[df_s140['sentiment'] == 0]['NB predictions'] == df_s140[df_s140['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == 0])\n",
    "NB_accuracy_s140_pos = 100.0*sum(df_s140[df_s140['sentiment'] == 1]['NB predictions'] == df_s140[df_s140['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == 1])\n",
    "## SA set\n",
    "NB_accuracy_sa = 100*float(sum(df_sa['NB predictions'] == df_sa['sentiment']))/len(df_sa)\n",
    "NB_accuracy_sa_neg = 100.0*sum(df_sa[df_sa['sentiment'] == -1]['NB predictions'] == df_sa[df_sa['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == -1])\n",
    "NB_accuracy_sa_neutral = 100.0*sum(df_sa[df_sa['sentiment'] == 0]['NB predictions'] == df_sa[df_sa['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == 0])\n",
    "NB_accuracy_sa_pos = 100.0*sum(df_sa[df_sa['sentiment'] == 1]['NB predictions'] == df_sa[df_sa['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == 1])\n",
    "\n",
    "## examine performance\n",
    "print 'Naive Bayes Performance:'\n",
    "pt = PrettyTable(field_names=['data set', 'accuracy', '(-) accuracy', '(0) accuracy', '(+) accuracy']) \n",
    "[ pt.add_row(['CV', round(NB_accuracy_cv, 2), round(NB_accuracy_cv_neg, 2), \\\n",
    "              round(NB_accuracy_cv_neutral, 2), round(NB_accuracy_cv_pos, 2) ])]\n",
    "\n",
    "[ pt.add_row(['S140', round(NB_accuracy_s140, 2), round(NB_accuracy_s140_neg, 2), \\\n",
    "              round(NB_accuracy_s140_neutral, 2), round(NB_accuracy_s140_pos, 2) ])]\n",
    "\n",
    "[ pt.add_row(['SA', round(NB_accuracy_sa, 2), round(NB_accuracy_sa_neg, 2), \\\n",
    "              round(NB_accuracy_sa_neutral, 2), round(NB_accuracy_sa_pos, 2) ])]\n",
    "\n",
    "#pt.align['Word'], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save the NB model variables\n",
    "np.save(path + 'words_unique', words_unique)\n",
    "np.save(path + 'prior_neg', prior_neg)\n",
    "np.save(path + 'prior_neutral', prior_neutral)\n",
    "np.save(path + 'prior_pos', prior_pos)\n",
    "np.save(path + 'ranks_tot', ranks_tot)\n",
    "np.save(path + 'ranks_tot_neg', ranks_tot_neg)\n",
    "np.save(path + 'ranks_tot_neutral', ranks_tot_neutral)\n",
    "np.save(path + 'ranks_tot_pos', ranks_tot_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "## MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create the sparse feature matrices\n",
    "def feature_matrix(df):\n",
    "  feature = scipy.sparse.lil_matrix((len(df), max_rank))\n",
    "  for j in range(len(df)):\n",
    "    for i in range(len(df['word_ranks'].iloc[j])):\n",
    "      feature[j, df['word_ranks'].iloc[j][i]] += 1.0/(len(df['word_ranks'].iloc[j]))\n",
    "  return scipy.sparse.csr_matrix(feature) ## convert into csr matrix\n",
    "\n",
    "feature = feature_matrix(df)\n",
    "feature_cv = feature_matrix(df_cv)\n",
    "feature_s140 = feature_matrix(df_s140)\n",
    "feature_sa = feature_matrix(df_sa)\n",
    "\n",
    "## define a special function to save it\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "save_sparse_csr(path + 'feature', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## useful functions\n",
    "\n",
    "## compute the average log likelihood\n",
    "def loglike(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta):\n",
    "  ## class indicator functions\n",
    "  I_neg = np.array(df['sentiment']== -1, dtype=float)\n",
    "  I_neutral = np.array(df['sentiment']== 0, dtype=float)\n",
    "  I_pos = np.array(df['sentiment']== 1, dtype=float)\n",
    "  ## useful factors\n",
    "  flambda_neg = feature.dot(lambda_neg)\n",
    "  flambda_neutral = feature.dot(lambda_neutral)\n",
    "  flambda_pos = feature.dot(lambda_pos)\n",
    "  return (flambda_neg.dot(I_neg) + flambda_neutral.dot(I_neutral) + flambda_pos.dot(I_pos) \\\n",
    "    - sum(np.log(np.exp(flambda_neg) + np.exp(flambda_neutral) + np.exp(flambda_pos))))/len(df) \\\n",
    "    - (beta/2)*sum(lambda_neg*lambda_neg + lambda_neutral*lambda_neutral + lambda_pos*lambda_pos)/len(df)\n",
    "\n",
    "## compute the gradient \n",
    "def gradient(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta):\n",
    "  ## class indicator functions\n",
    "  I_neg = np.array(df['sentiment']== -1, dtype=float)\n",
    "  I_neutral = np.array(df['sentiment']== 0, dtype=float)\n",
    "  I_pos = np.array(df['sentiment']== 1, dtype=float)\n",
    "  ## useful factors\n",
    "  flambda_neg = feature.dot(lambda_neg)\n",
    "  flambda_neutral = feature.dot(lambda_neutral)\n",
    "  flambda_pos = feature.dot(lambda_pos)\n",
    "  ## boltzmann factors\n",
    "  boltz_neg = np.exp(flambda_neg)\n",
    "  boltz_neutral = np.exp(flambda_neutral)\n",
    "  boltz_pos = np.exp(flambda_pos)\n",
    "  ## the gradients\n",
    "  dLLdlambda_neg = (feature.transpose().dot(I_neg) \\\n",
    "                    - feature.transpose().dot( (boltz_neg/(boltz_neg + boltz_neutral + boltz_pos))))/len(df)\n",
    "  dLLdlambda_neutral = (feature.transpose().dot(I_neutral) \\\n",
    "                        - feature.transpose().dot( (boltz_neutral/(boltz_neg + boltz_neutral + boltz_pos))))/len(df)\n",
    "  dLLdlambda_pos = (feature.transpose().dot(I_pos) \\\n",
    "                    - feature.transpose().dot( (boltz_pos/(boltz_neg + boltz_neutral + boltz_pos))))/len(df)\n",
    "  ## add L2 regularization\n",
    "  dLLdlambda_neg = dLLdlambda_neg - beta*lambda_neg/len(df)\n",
    "  dLLdlambda_neutral = dLLdlambda_neutral - beta*lambda_neutral/len(df)\n",
    "  dLLdlambda_pos = dLLdlambda_pos - beta*lambda_pos/len(df)  \n",
    "  return [dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos]\n",
    "\n",
    "## prediction accuracy\n",
    "def ME_predictions(df, feature, lambda_neg, lambda_neutral, lambda_pos):\n",
    "  ## useful factors\n",
    "  flambda_neg = feature.dot(lambda_neg)\n",
    "  flambda_neutral = feature.dot(lambda_neutral)\n",
    "  flambda_pos = feature.dot(lambda_pos)\n",
    "  ## boltzmann factors\n",
    "  boltz_neg = np.squeeze(np.asarray(np.exp(flambda_neg)))\n",
    "  boltz_neutral = np.squeeze(np.asarray(np.exp(flambda_neutral)))\n",
    "  boltz_pos = np.squeeze(np.asarray(np.exp(flambda_pos)))\n",
    "  predict = []\n",
    "  for i in range(len(df)):\n",
    "    p_neg = boltz_neg[i]\n",
    "    p_neutral = boltz_neutral[i]\n",
    "    p_pos = boltz_pos[i]\n",
    "    predict.append(-1 + [p_neg, p_neutral, p_pos].index(max([p_neg, p_neutral, p_pos])))\n",
    "  return predict\n",
    "\n",
    "## compute gradient via finite differences\n",
    "def gradient_FD(lambda_neg, lambda_neutral, lambda_pos, epsilon, beta):\n",
    "  LL1 = loglike(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)\n",
    "  dLLdlambda_neg_FD = [0 for i in range(max_rank)]\n",
    "  dLLdlambda_neutral_FD = [0 for i in range(max_rank)]\n",
    "  dLLdlambda_pos_FD = [0 for i in range(max_rank)]\n",
    "  \n",
    "  ## negative\n",
    "  for i in range(len(lambda_neg)):\n",
    "    lambda_neg2 = np.copy(lambda_neg)\n",
    "    lambda_neutral2 = np.copy(lambda_neutral)\n",
    "    lambda_pos2 = np.copy(lambda_pos)\n",
    "    lambda_neg2[i] += epsilon #update only the neg lambda    \n",
    "    LL2 = loglike(df, feature, lambda_neg2, lambda_neutral2, lambda_pos2, beta)\n",
    "    dLLdlambda_neg_FD[i] = (LL2 - LL1)/epsilon\n",
    "    \n",
    "  ## neutral\n",
    "  for i in range(len(lambda_neg)):\n",
    "    lambda_neg2 = np.copy(lambda_neg)\n",
    "    lambda_neutral2 = np.copy(lambda_neutral)\n",
    "    lambda_pos2 = np.copy(lambda_pos)\n",
    "    lambda_neutral2[i] += epsilon #update only the neutral lambda    \n",
    "    LL2 = loglike(df, feature, lambda_neg2, lambda_neutral2, lambda_pos2, beta)\n",
    "    dLLdlambda_neutral_FD[i] = (LL2 - LL1)/epsilon\n",
    "\n",
    "  ## positive\n",
    "  for i in range(len(lambda_neg)):\n",
    "    lambda_neg2 = np.copy(lambda_neg)\n",
    "    lambda_neutral2 = np.copy(lambda_neutral)\n",
    "    lambda_pos2 = np.copy(lambda_pos)\n",
    "    lambda_pos2[i] += epsilon #update only the pos lambda    \n",
    "    LL2 = loglike(df, feature, lambda_neg2, lambda_neutral2, lambda_pos2, beta)\n",
    "    dLLdlambda_pos_FD[i] = (LL2 - LL1)/epsilon\n",
    "    \n",
    "  return [dLLdlambda_neg_FD, dLLdlambda_neutral_FD, dLLdlambda_pos_FD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## check the gradient against finite differences (only use for small data sets, takes long time otherwise)\n",
    "gradcheck = False\n",
    "\n",
    "if gradcheck:\n",
    "  beta = 11.0\n",
    "  grad_ME = gradient(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)\n",
    "  np.random.seed(seed=1)\n",
    "  lambda_neg = np.random.uniform(0,1,max_rank)\n",
    "  lambda_neutral = np.random.uniform(0,1,max_rank)\n",
    "  lambda_pos = np.random.uniform(0,1,max_rank)\n",
    "\n",
    "  ## check to see that the difference scales like epsilon\n",
    "  grad_FD = gradient_FD(lambda_neg, lambda_neutral, lambda_pos, 0.01, beta)\n",
    "  print [ sum(grad_ME[0] - grad_FD[0])/len(grad_ME[0]), sum(grad_ME[1] - grad_FD[1])/len(grad_ME[1]), \\\n",
    "       sum(grad_ME[2] - grad_FD[2])/len(grad_ME[2])]\n",
    "\n",
    "  grad_FD = gradient_FD(lambda_neg, lambda_neutral, lambda_pos, 0.001, beta)\n",
    "  print [ sum(grad_ME[0] - grad_FD[0])/len(grad_ME[0]), sum(grad_ME[1] - grad_FD[1])/len(grad_ME[1]), \\\n",
    "       sum(grad_ME[2] - grad_FD[2])/len(grad_ME[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625]\n"
     ]
    }
   ],
   "source": [
    "beta_list = [(2)**(1-j) for j in range(10)]\n",
    "print beta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta =  2\n",
      "i =  0 LL decreased LL =  -1.10568489607 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21883867125 max_grad= 0.0438606586018 decaying alpha to =  281.25\n",
      "i =  560 entered zigzag LL =  -0.67671273822 max_grad= 0.0225961644163 decaying alpha to =  210.9375\n",
      "i =  600 LL decreased LL =  -0.599230737784 max_grad= 0.00680456917903 decaying alpha to =  158.203125\n",
      "i= 1000 LL= [-0.58863610904338104, -0.56442760789915492] max_grad= 1.27647485669e-05\n",
      "i= 2000 LL= [-0.58385984799530444, -0.55313921558585799] max_grad= 7.37683362975e-06\n",
      "i= 3000 LL= [-0.58252597763408998, -0.5482970356901854] max_grad= 4.17327570926e-06\n",
      "i= 4000 LL= [-0.58207674352810113, -0.54589324023515107] max_grad= 2.34574878886e-06\n",
      "i= 5000 LL= [-0.58191023448909418, -0.54460518346220987] max_grad= 1.31722679089e-06\n",
      "\n",
      "training halted: gradients converged\n",
      "training iterations: 5481 LL= [-0.58187146369005238, -0.54420751436653558]\n",
      "max_grad:  9.98413626328e-07 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  1\n",
      "i =  0 LL decreased LL =  -1.10525096455 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21695157037 max_grad= 0.0438395033329 decaying alpha to =  281.25\n",
      "i =  670 entered zigzag LL =  -0.630543627789 max_grad= 0.0217052583254 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.55121105934124082, -0.55945037204816461] max_grad= 0.00790396441508\n",
      "i =  1050 entered zigzag LL =  -0.5499841944 max_grad= 0.00795293025099 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.53076974027238311, -0.54039451731882882] max_grad= 1.07111182285e-05\n",
      "i= 3000 LL= [-0.52312187671844268, -0.53379655815896798] max_grad= 7.32229881669e-06\n",
      "i= 4000 LL= [-0.51834824355090736, -0.52981037518616303] max_grad= 5.00622492847e-06\n",
      "i= 5000 LL= [-0.51515813383702569, -0.52722197949734317] max_grad= 3.43700718755e-06\n",
      "i= 6000 LL= [-0.51293301712481887, -0.5254633303928965] max_grad= 2.37224037267e-06\n",
      "i= 7000 LL= [-0.51133489592367498, -0.52423060576942349] max_grad= 1.64640430701e-06\n",
      "i= 8000 LL= [-0.5101624552094679, -0.52334660687159218] max_grad= 1.1489315997e-06\n",
      "\n",
      "training halted: gradients converged\n",
      "training iterations: 8391 LL= [-0.50979144349857353, -0.52307118633555094]\n",
      "max_grad:  9.99638774346e-07 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.5\n",
      "i =  0 LL decreased LL =  -1.10535944743 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21686047357 max_grad= 0.0438289147285 decaying alpha to =  281.25\n",
      "i =  750 entered zigzag LL =  -0.628894528423 max_grad= 0.0211522787581 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.55574550316999005, -0.55304467992301365] max_grad= 0.00674431267637\n",
      "i =  1150 entered zigzag LL =  -0.552887960286 max_grad= 0.00697961490148 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.53902054794908871, -0.53390773983719853] max_grad= 1.34161367707e-05\n",
      "i= 3000 LL= [-0.53309219696874577, -0.52610143670857279] max_grad= 1.01272536233e-05\n",
      "i= 4000 LL= [-0.52952528611793226, -0.52096856828725324] max_grad= 7.68942034866e-06\n",
      "i= 5000 LL= [-0.52721209496222265, -0.51733530611495904] max_grad= 5.89338255353e-06\n",
      "i= 6000 LL= [-0.52563862080184109, -0.51464070659028938] max_grad= 4.5603800844e-06\n",
      "i= 7000 LL= [-0.52453231312403714, -0.5125768062738586] max_grad= 3.55987415302e-06\n",
      "i= 8000 LL= [-0.52373536539072907, -0.51095825795624683] max_grad= 2.80027467271e-06\n",
      "i= 9000 LL= [-0.52315052679952068, -0.50966592793379328] max_grad= 2.21749504173e-06\n",
      "i= 10000 LL= [-0.52271502244252532, -0.50861936047958978] max_grad= 1.76622732037e-06\n",
      "i= 11000 LL= [-0.52238686349776808, -0.50776208887052721] max_grad= 1.41397674517e-06\n",
      "i= 12000 LL= [-0.52213716595974813, -0.50705324591094714] max_grad= 1.13709143799e-06\n",
      "\n",
      "training halted: gradients converged\n",
      "training iterations: 12601 LL= [-0.52201629084637746, -0.50668623589074413]\n",
      "max_grad:  9.99461125686e-07 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.25\n",
      "i =  0 LL decreased LL =  -1.10530520599 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21652988671 max_grad= 0.0438236176767 decaying alpha to =  281.25\n",
      "i =  800 entered zigzag LL =  -0.617787795657 max_grad= 0.0208357882628 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.54666651284590961, -0.54960234644518846] max_grad= 0.00590505197628\n",
      "i =  1210 entered zigzag LL =  -0.542162542468 max_grad= 0.00637781593992 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.52811726560525507, -0.53057769947368127] max_grad= 1.50729231757e-05\n",
      "i= 3000 LL= [-0.52016178392605883, -0.52219081624494734] max_grad= 1.19741143565e-05\n",
      "i= 4000 LL= [-0.51487061140181523, -0.51647196395855821] max_grad= 9.60489205831e-06\n",
      "i= 5000 LL= [-0.5110891064688714, -0.51227140389998405] max_grad= 7.80454908279e-06\n",
      "i= 6000 LL= [-0.5082598789553161, -0.50903615318142204] max_grad= 6.42200323748e-06\n",
      "i= 7000 LL= [-0.50607492682263944, -0.50646069199985555] max_grad= 5.3436346688e-06\n",
      "i= 8000 LL= [-0.50434790668332408, -0.50435994455563271] max_grad= 4.48895838118e-06\n",
      "i= 9000 LL= [-0.50295845573479925, -0.5026140992589051] max_grad= 3.80150147028e-06\n",
      "i= 10000 LL= [-0.50182489300985045, -0.50114158680283638] max_grad= 3.24127885751e-06\n",
      "i= 11000 LL= [-0.5008896148881834, -0.49988463230859903] max_grad= 2.77952820809e-06\n",
      "i= 12000 LL= [-0.50011073324675559, -0.49880098853917737] max_grad= 2.39519006188e-06\n",
      "i= 13000 LL= [-0.49945701953684485, -0.4978589418491971] max_grad= 2.07257117352e-06\n",
      "i= 14000 LL= [-0.49890470775780293, -0.49703415757103497] max_grad= 1.79978074155e-06\n",
      "i= 15000 LL= [-0.49843539598931935, -0.49630761167087095] max_grad= 1.56766899895e-06\n",
      "i= 16000 LL= [-0.49803462497587725, -0.49566419123437011] max_grad= 1.36909500552e-06\n",
      "i= 17000 LL= [-0.4976908891766697, -0.49509172168241067] max_grad= 1.19841312966e-06\n",
      "i= 18000 LL= [-0.49739493265647933, -0.49458027468797183] max_grad= 1.05110709378e-06\n",
      "\n",
      "training halted: gradients converged\n",
      "training iterations: 18391 LL= [-0.49729078866172888, -0.49439553467052766]\n",
      "max_grad:  9.99051968698e-07 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.125\n",
      "i =  0 LL decreased LL =  -1.10527808527 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21636449795 max_grad= 0.0438209684626 decaying alpha to =  281.25\n",
      "i =  830 entered zigzag LL =  -0.611565491449 max_grad= 0.0206613408815 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.54167143171154075, -0.54774430528847284] max_grad= 0.00531225127492\n",
      "i =  1250 entered zigzag LL =  -0.536013989681 max_grad= 0.0060322729983 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.52201111049508608, -0.52886630988676719] max_grad= 1.5986028699e-05\n",
      "i= 3000 LL= [-0.51276688824781036, -0.52022321301502095] max_grad= 1.3034390514e-05\n",
      "i= 4000 LL= [-0.50629324631614148, -0.51423560390481449] max_grad= 1.07546848571e-05\n",
      "i= 5000 LL= [-0.50142952596133294, -0.50976851329909845] max_grad= 9.00691081414e-06\n",
      "i= 6000 LL= [-0.49760781815411864, -0.50627390272913519] max_grad= 7.65155463108e-06\n",
      "i= 7000 LL= [-0.49451025590709402, -0.5034479023413756] max_grad= 6.58202333836e-06\n",
      "i= 8000 LL= [-0.49194200626499035, -0.50110584125972835] max_grad= 5.72253436962e-06\n",
      "i= 9000 LL= [-0.48977548104185908, -0.49912779720193934] max_grad= 5.01997955595e-06\n",
      "i= 10000 LL= [-0.48792281098253881, -0.49743187184424148] max_grad= 4.43688061522e-06\n",
      "i= 11000 LL= [-0.48632104084056105, -0.4959598796047256] max_grad= 3.94639055533e-06\n",
      "i= 12000 LL= [-0.48492361100703035, -0.49466914788315486] max_grad= 3.52893926962e-06\n",
      "i= 13000 LL= [-0.48369518136802497, -0.49352755486470518] max_grad= 3.17000722228e-06\n",
      "i= 14000 LL= [-0.48260834091785038, -0.49251038772707023] max_grad= 2.85863840687e-06\n",
      "i =  14540 entered zigzag LL =  -0.482072509633 max_grad= 2.70723716067e-06 decaying alpha to =  118.65234375\n",
      "i =  14560 entered zigzag LL =  -0.482059055562 max_grad= 2.70338592394e-06 decaying alpha to =  88.9892578125\n",
      "i =  14580 entered zigzag LL =  -0.482048979155 max_grad= 2.70050221886e-06 decaying alpha to =  66.7419433594\n",
      "i =  14590 entered zigzag LL =  -0.482045473251 max_grad= 2.69947871917e-06 decaying alpha to =  50.0564575195\n",
      "i =  14610 entered zigzag LL =  -0.482039812972 max_grad= 2.69785922237e-06 decaying alpha to =  37.5423431396\n",
      "i =  14620 entered zigzag LL =  -0.48203784288 max_grad= 2.69728418255e-06 decaying alpha to =  28.1567573547\n",
      "i =  14630 entered zigzag LL =  -0.482036365618 max_grad= 2.69685300901e-06 decaying alpha to =  21.1175680161\n",
      "i =  14640 entered zigzag LL =  -0.482035257842 max_grad= 2.69652968865e-06 decaying alpha to =  15.838176012\n",
      "i =  14650 entered zigzag LL =  -0.482034427107 max_grad= 2.69628723201e-06 decaying alpha to =  11.878632009\n",
      "i =  14660 entered zigzag LL =  -0.48203380411 max_grad= 2.69610540844e-06 decaying alpha to =  8.90897400677\n",
      "i =  14680 entered zigzag LL =  -0.482032797829 max_grad= 2.69581755413e-06 decaying alpha to =  6.68173050508\n",
      "i =  14690 entered zigzag LL =  -0.482032447455 max_grad= 2.69571529983e-06 decaying alpha to =  5.01129787881\n",
      "i =  14700 entered zigzag LL =  -0.482032184685 max_grad= 2.69563861247e-06 decaying alpha to =  3.75847340911\n",
      "i =  14710 entered zigzag LL =  -0.482031987612 max_grad= 2.69558109884e-06 decaying alpha to =  2.81885505683\n",
      "i =  14720 entered zigzag LL =  -0.482031839811 max_grad= 2.69553796468e-06 decaying alpha to =  2.11414129262\n",
      "i =  14730 entered zigzag LL =  -0.482031728962 max_grad= 2.69550561467e-06 decaying alpha to =  1.58560596947\n",
      "i =  14740 entered zigzag LL =  -0.482031645827 max_grad= 2.69548135249e-06 decaying alpha to =  1.1892044771\n",
      "i =  14750 entered zigzag LL =  -0.482031583475 max_grad= 2.69546315605e-06 decaying alpha to =  0.891903357825\n",
      "i =  14760 entered zigzag LL =  -0.482031536712 max_grad= 2.69544950882e-06 decaying alpha to =  0.668927518369\n",
      "i =  14770 entered zigzag LL =  -0.48203150164 max_grad= 2.69543927346e-06 decaying alpha to =  0.501695638777\n",
      "i =  14780 entered zigzag LL =  -0.482031475335 max_grad= 2.69543159697e-06 decaying alpha to =  0.376271729083\n",
      "i =  14790 entered zigzag LL =  -0.482031455608 max_grad= 2.69542583963e-06 decaying alpha to =  0.282203796812\n",
      "i =  14800 entered zigzag LL =  -0.482031440812 max_grad= 2.69542152163e-06 decaying alpha to =  0.211652847609\n",
      "i =  14810 entered zigzag LL =  -0.482031429715 max_grad= 2.69541828314e-06 decaying alpha to =  0.158739635707\n",
      "i =  14820 entered zigzag LL =  -0.482031421392 max_grad= 2.69541585427e-06 decaying alpha to =  0.11905472678\n",
      "i =  14830 entered zigzag LL =  -0.48203141515 max_grad= 2.69541403262e-06 decaying alpha to =  0.089291045085\n",
      "i =  14840 entered zigzag LL =  -0.482031410468 max_grad= 2.69541266639e-06 decaying alpha to =  0.0669682838138\n",
      "i =  14850 entered zigzag LL =  -0.482031406957 max_grad= 2.69541164171e-06 decaying alpha to =  0.0502262128603\n",
      "i =  14860 entered zigzag LL =  -0.482031404324 max_grad= 2.69541087321e-06 decaying alpha to =  0.0376696596452\n",
      "i =  14870 entered zigzag LL =  -0.482031402348 max_grad= 2.69541029683e-06 decaying alpha to =  0.0282522447339\n",
      "i =  14880 entered zigzag LL =  -0.482031400868 max_grad= 2.69540986454e-06 decaying alpha to =  0.0211891835504\n",
      "i =  14890 entered zigzag LL =  -0.482031399757 max_grad= 2.69540954033e-06 decaying alpha to =  0.0158918876628\n",
      "i =  14900 entered zigzag LL =  -0.482031398924 max_grad= 2.69540929717e-06 decaying alpha to =  0.0119189157471\n",
      "i =  14910 entered zigzag LL =  -0.482031398299 max_grad= 2.6954091148e-06 decaying alpha to =  0.00893918681034\n",
      "i =  14920 entered zigzag LL =  -0.48203139783 max_grad= 2.69540897802e-06 decaying alpha to =  0.00670439010776\n",
      "i =  14930 entered zigzag LL =  -0.482031397478 max_grad= 2.69540887544e-06 decaying alpha to =  0.00502829258082\n",
      "i =  14940 entered zigzag LL =  -0.482031397215 max_grad= 2.6954087985e-06 decaying alpha to =  0.00377121943561\n",
      "i =  14950 entered zigzag LL =  -0.482031397017 max_grad= 2.6954087408e-06 decaying alpha to =  0.00282841457671\n",
      "i =  14960 entered zigzag LL =  -0.482031396868 max_grad= 2.69540869752e-06 decaying alpha to =  0.00212131093253\n",
      "i =  14970 entered zigzag LL =  -0.482031396758 max_grad= 2.69540866506e-06 decaying alpha to =  0.0015909831994\n",
      "i =  14980 entered zigzag LL =  -0.482031396674 max_grad= 2.69540864072e-06 decaying alpha to =  0.00119323739955\n",
      "i =  14990 entered zigzag LL =  -0.482031396611 max_grad= 2.69540862246e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 14991\n",
      "LL= [-0.48203333689278632, -0.49196888917549059]\n",
      "max_grad:  2.69540862246e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.0625\n",
      "i =  0 LL decreased LL =  -1.10526452491 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21628177972 max_grad= 0.0438196436833 decaying alpha to =  281.25\n",
      "i =  850 entered zigzag LL =  -0.608042572959 max_grad= 0.0205623104385 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.53891372368823609, -0.54665666341353558] max_grad= 0.0048445081304\n",
      "i =  1270 entered zigzag LL =  -0.532720898586 max_grad= 0.00583071982733 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.51875511462810819, -0.52799165344304633] max_grad= 1.64649074157e-05\n",
      "i= 3000 LL= [-0.50878292032769978, -0.51923487966056525] max_grad= 1.36027546445e-05\n",
      "i= 4000 LL= [-0.5016136147978445, -0.5131249173059349] max_grad= 1.13856944299e-05\n",
      "i= 5000 LL= [-0.4960883400161884, -0.50853551647265727] max_grad= 9.68313128771e-06\n",
      "i= 6000 LL= [-0.49163699850503512, -0.50492162104438587] max_grad= 8.36072325157e-06\n",
      "i= 7000 LL= [-0.4879390599251186, -0.50198037346589708] max_grad= 7.31496729859e-06\n",
      "i= 8000 LL= [-0.48479726247626692, -0.49952735771711526] max_grad= 6.47212823114e-06\n",
      "i= 9000 LL= [-0.482081921063365, -0.49744258563977167] max_grad= 5.78055560818e-06\n",
      "i= 10000 LL= [-0.47970336805586, -0.49564395424876162] max_grad= 5.20385456763e-06\n",
      "i =  10260 entered zigzag LL =  -0.479131721127 max_grad= 5.06933507187e-06 decaying alpha to =  118.65234375\n",
      "i =  10280 entered zigzag LL =  -0.479101441925 max_grad= 5.06212759819e-06 decaying alpha to =  88.9892578125\n",
      "i =  10300 entered zigzag LL =  -0.479078765652 max_grad= 5.05673304996e-06 decaying alpha to =  66.7419433594\n",
      "i =  10310 entered zigzag LL =  -0.479070876114 max_grad= 5.05481884529e-06 decaying alpha to =  50.0564575195\n",
      "i =  10330 entered zigzag LL =  -0.479058138732 max_grad= 5.05179046291e-06 decaying alpha to =  37.5423431396\n",
      "i =  10340 entered zigzag LL =  -0.479053705498 max_grad= 5.05071530958e-06 decaying alpha to =  28.1567573547\n",
      "i =  10350 entered zigzag LL =  -0.479050381286 max_grad= 5.0499091929e-06 decaying alpha to =  21.1175680161\n",
      "i =  10360 entered zigzag LL =  -0.47904788853 max_grad= 5.049304745e-06 decaying alpha to =  15.838176012\n",
      "i =  10370 entered zigzag LL =  -0.479046019188 max_grad= 5.04885148758e-06 decaying alpha to =  11.878632009\n",
      "i =  10380 entered zigzag LL =  -0.479044617309 max_grad= 5.04851158867e-06 decaying alpha to =  8.90897400677\n",
      "i =  10400 entered zigzag LL =  -0.479042352965 max_grad= 5.04797349251e-06 decaying alpha to =  6.68173050508\n",
      "i =  10410 entered zigzag LL =  -0.479041564555 max_grad= 5.04778234951e-06 decaying alpha to =  5.01129787881\n",
      "i =  10420 entered zigzag LL =  -0.479040973269 max_grad= 5.04763900011e-06 decaying alpha to =  3.75847340911\n",
      "i =  10430 entered zigzag LL =  -0.479040529818 max_grad= 5.04753149248e-06 decaying alpha to =  2.81885505683\n",
      "i =  10440 entered zigzag LL =  -0.479040197236 max_grad= 5.04745086424e-06 decaying alpha to =  2.11414129262\n",
      "i =  10450 entered zigzag LL =  -0.479039947805 max_grad= 5.04739039446e-06 decaying alpha to =  1.58560596947\n",
      "i =  10460 entered zigzag LL =  -0.479039760733 max_grad= 5.04734504291e-06 decaying alpha to =  1.1892044771\n",
      "i =  10470 entered zigzag LL =  -0.47903962043 max_grad= 5.04731102969e-06 decaying alpha to =  0.891903357825\n",
      "i =  10480 entered zigzag LL =  -0.479039515204 max_grad= 5.04728552002e-06 decaying alpha to =  0.668927518369\n",
      "i =  10490 entered zigzag LL =  -0.479039436285 max_grad= 5.04726638791e-06 decaying alpha to =  0.501695638777\n",
      "i =  10500 entered zigzag LL =  -0.479039377096 max_grad= 5.04725203891e-06 decaying alpha to =  0.376271729083\n",
      "i =  10510 entered zigzag LL =  -0.479039332704 max_grad= 5.0472412772e-06 decaying alpha to =  0.282203796812\n",
      "i =  10520 entered zigzag LL =  -0.479039299411 max_grad= 5.04723320594e-06 decaying alpha to =  0.211652847609\n",
      "i =  10530 entered zigzag LL =  -0.47903927444 max_grad= 5.04722715252e-06 decaying alpha to =  0.158739635707\n",
      "i =  10540 entered zigzag LL =  -0.479039255713 max_grad= 5.04722261245e-06 decaying alpha to =  0.11905472678\n",
      "i =  10550 entered zigzag LL =  -0.479039241667 max_grad= 5.04721920741e-06 decaying alpha to =  0.089291045085\n",
      "i =  10560 entered zigzag LL =  -0.479039231132 max_grad= 5.04721665363e-06 decaying alpha to =  0.0669682838138\n",
      "i =  10570 entered zigzag LL =  -0.479039223232 max_grad= 5.0472147383e-06 decaying alpha to =  0.0502262128603\n",
      "i =  10580 entered zigzag LL =  -0.479039217306 max_grad= 5.0472133018e-06 decaying alpha to =  0.0376696596452\n",
      "i =  10590 entered zigzag LL =  -0.479039212862 max_grad= 5.04721222442e-06 decaying alpha to =  0.0282522447339\n",
      "i =  10600 entered zigzag LL =  -0.479039209529 max_grad= 5.04721141639e-06 decaying alpha to =  0.0211891835504\n",
      "i =  10610 entered zigzag LL =  -0.479039207029 max_grad= 5.04721081037e-06 decaying alpha to =  0.0158918876628\n",
      "i =  10620 entered zigzag LL =  -0.479039205154 max_grad= 5.04721035585e-06 decaying alpha to =  0.0119189157471\n",
      "i =  10630 entered zigzag LL =  -0.479039203748 max_grad= 5.04721001496e-06 decaying alpha to =  0.00893918681034\n",
      "i =  10640 entered zigzag LL =  -0.479039202693 max_grad= 5.0472097593e-06 decaying alpha to =  0.00670439010776\n",
      "i =  10650 entered zigzag LL =  -0.479039201902 max_grad= 5.04720956755e-06 decaying alpha to =  0.00502829258082\n",
      "i =  10660 entered zigzag LL =  -0.479039201309 max_grad= 5.04720942374e-06 decaying alpha to =  0.00377121943561\n",
      "i =  10670 entered zigzag LL =  -0.479039200864 max_grad= 5.04720931588e-06 decaying alpha to =  0.00282841457671\n",
      "i =  10680 entered zigzag LL =  -0.479039200531 max_grad= 5.04720923498e-06 decaying alpha to =  0.00212131093253\n",
      "i =  10690 entered zigzag LL =  -0.47903920028 max_grad= 5.04720917431e-06 decaying alpha to =  0.0015909831994\n",
      "i =  10700 entered zigzag LL =  -0.479039200093 max_grad= 5.04720912881e-06 decaying alpha to =  0.00119323739955\n",
      "i =  10710 entered zigzag LL =  -0.479039199952 max_grad= 5.04720909468e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 10711\n",
      "LL= [-0.47904356597177494, -0.49514963865342748]\n",
      "max_grad:  5.04720909468e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.03125\n",
      "i =  0 LL decreased LL =  -1.10525774473 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21624041464 max_grad= 0.0438189812506 decaying alpha to =  281.25\n",
      "i =  860 entered zigzag LL =  -0.606249693657 max_grad= 0.0205123689174 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.53746187042070903, -0.54606714374953869] max_grad= 0.00453324876294\n",
      "i =  1280 entered zigzag LL =  -0.531030932393 max_grad= 0.00572662295651 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.51707877232154476, -0.5275561253172163] max_grad= 1.67123787495e-05\n",
      "i= 3000 LL= [-0.50671656833552625, -0.51874440759168894] max_grad= 1.38988726981e-05\n",
      "i= 4000 LL= [-0.49916784894362071, -0.51257580347532661] max_grad= 1.17178039033e-05\n",
      "i= 5000 LL= [-0.49327510008611691, -0.5079282322674894] max_grad= 1.00430776728e-05\n",
      "i= 6000 LL= [-0.48846753061708503, -0.50425805991338535] max_grad= 8.74277566307e-06\n",
      "i= 7000 LL= [-0.48442363152908086, -0.50126289863823226] max_grad= 7.71481861173e-06\n",
      "i= 8000 LL= [-0.48094524230659996, -0.49875843411552973] max_grad= 6.88640583497e-06\n",
      "i =  8970 entered zigzag LL =  -0.477987821139 max_grad= 6.22515501656e-06 decaying alpha to =  118.65234375\n",
      "i =  8990 entered zigzag LL =  -0.477947693717 max_grad= 6.21630446895e-06 decaying alpha to =  88.9892578125\n",
      "i= 9000 LL= [-0.4779337366913009, -0.49664664829302141] max_grad= 6.21316572091e-06\n",
      "i =  9010 entered zigzag LL =  -0.477917642561 max_grad= 6.20968159303e-06 decaying alpha to =  66.7419433594\n",
      "i =  9020 entered zigzag LL =  -0.477907187251 max_grad= 6.20733182773e-06 decaying alpha to =  50.0564575195\n",
      "i =  9040 entered zigzag LL =  -0.477890307628 max_grad= 6.20361468412e-06 decaying alpha to =  37.5423431396\n",
      "i =  9050 entered zigzag LL =  -0.477884432721 max_grad= 6.20229509747e-06 decaying alpha to =  28.1567573547\n",
      "i =  9060 entered zigzag LL =  -0.477880027499 max_grad= 6.20130574476e-06 decaying alpha to =  21.1175680161\n",
      "i =  9070 entered zigzag LL =  -0.477876724121 max_grad= 6.20056391986e-06 decaying alpha to =  15.838176012\n",
      "i =  9080 entered zigzag LL =  -0.477874246889 max_grad= 6.20000765782e-06 decaying alpha to =  11.878632009\n",
      "i =  9090 entered zigzag LL =  -0.477872389136 max_grad= 6.19959052125e-06 decaying alpha to =  8.90897400677\n",
      "i =  9110 entered zigzag LL =  -0.477869388458 max_grad= 6.19893015979e-06 decaying alpha to =  6.68173050508\n",
      "i =  9120 entered zigzag LL =  -0.477868343668 max_grad= 6.19869558858e-06 decaying alpha to =  5.01129787881\n",
      "i =  9130 entered zigzag LL =  -0.477867560106 max_grad= 6.19851967084e-06 decaying alpha to =  3.75847340911\n",
      "i =  9140 entered zigzag LL =  -0.477866972451 max_grad= 6.19838773853e-06 decaying alpha to =  2.81885505683\n",
      "i =  9150 entered zigzag LL =  -0.47786653172 max_grad= 6.19828879268e-06 decaying alpha to =  2.11414129262\n",
      "i =  9160 entered zigzag LL =  -0.477866201177 max_grad= 6.19821458519e-06 decaying alpha to =  1.58560596947\n",
      "i =  9170 entered zigzag LL =  -0.477865953273 max_grad= 6.19815893063e-06 decaying alpha to =  1.1892044771\n",
      "i =  9180 entered zigzag LL =  -0.477865767346 max_grad= 6.19811719032e-06 decaying alpha to =  0.891903357825\n",
      "i =  9190 entered zigzag LL =  -0.477865627902 max_grad= 6.19808588542e-06 decaying alpha to =  0.668927518369\n",
      "i =  9200 entered zigzag LL =  -0.47786552332 max_grad= 6.19806240694e-06 decaying alpha to =  0.501695638777\n",
      "i =  9210 entered zigzag LL =  -0.477865444883 max_grad= 6.19804479819e-06 decaying alpha to =  0.376271729083\n",
      "i =  9220 entered zigzag LL =  -0.477865386056 max_grad= 6.19803159168e-06 decaying alpha to =  0.282203796812\n",
      "i =  9230 entered zigzag LL =  -0.477865341936 max_grad= 6.19802168683e-06 decaying alpha to =  0.211652847609\n",
      "i =  9240 entered zigzag LL =  -0.477865308845 max_grad= 6.19801425822e-06 decaying alpha to =  0.158739635707\n",
      "i =  9250 entered zigzag LL =  -0.477865284028 max_grad= 6.19800868677e-06 decaying alpha to =  0.11905472678\n",
      "i =  9260 entered zigzag LL =  -0.477865265414 max_grad= 6.19800450818e-06 decaying alpha to =  0.089291045085\n",
      "i =  9270 entered zigzag LL =  -0.477865251455 max_grad= 6.19800137425e-06 decaying alpha to =  0.0669682838138\n",
      "i =  9280 entered zigzag LL =  -0.477865240985 max_grad= 6.1979990238e-06 decaying alpha to =  0.0502262128603\n",
      "i =  9290 entered zigzag LL =  -0.477865233132 max_grad= 6.19799726097e-06 decaying alpha to =  0.0376696596452\n",
      "i =  9300 entered zigzag LL =  -0.477865227243 max_grad= 6.19799593884e-06 decaying alpha to =  0.0282522447339\n",
      "i =  9310 entered zigzag LL =  -0.477865222826 max_grad= 6.19799494725e-06 decaying alpha to =  0.0211891835504\n",
      "i =  9320 entered zigzag LL =  -0.477865219513 max_grad= 6.19799420355e-06 decaying alpha to =  0.0158918876628\n",
      "i =  9330 entered zigzag LL =  -0.477865217029 max_grad= 6.19799364578e-06 decaying alpha to =  0.0119189157471\n",
      "i =  9340 entered zigzag LL =  -0.477865215165 max_grad= 6.19799322745e-06 decaying alpha to =  0.00893918681034\n",
      "i =  9350 entered zigzag LL =  -0.477865213768 max_grad= 6.19799291371e-06 decaying alpha to =  0.00670439010776\n",
      "i =  9360 entered zigzag LL =  -0.47786521272 max_grad= 6.1979926784e-06 decaying alpha to =  0.00502829258082\n",
      "i =  9370 entered zigzag LL =  -0.477865211933 max_grad= 6.19799250192e-06 decaying alpha to =  0.00377121943561\n",
      "i =  9380 entered zigzag LL =  -0.477865211344 max_grad= 6.19799236955e-06 decaying alpha to =  0.00282841457671\n",
      "i =  9390 entered zigzag LL =  -0.477865210902 max_grad= 6.19799227028e-06 decaying alpha to =  0.00212131093253\n",
      "i =  9400 entered zigzag LL =  -0.477865210571 max_grad= 6.19799219583e-06 decaying alpha to =  0.0015909831994\n",
      "i =  9410 entered zigzag LL =  -0.477865210322 max_grad= 6.19799213999e-06 decaying alpha to =  0.00119323739955\n",
      "i =  9420 entered zigzag LL =  -0.477865210135 max_grad= 6.19799209811e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 9421\n",
      "LL= [-0.4778709959172715, -0.4966032268086632]\n",
      "max_grad:  6.19799209811e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.015625\n",
      "i =  0 LL decreased LL =  -1.10525435464 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21621973061 max_grad= 0.0438186500235 decaying alpha to =  281.25\n",
      "i =  860 entered zigzag LL =  -0.605648134317 max_grad= 0.0204973469033 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.53687325002318709, -0.54590374263802632] max_grad= 0.0044833648835\n",
      "i =  1280 entered zigzag LL =  -0.530354758214 max_grad= 0.00568986285539 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.51627874686038233, -0.52738167236198752] max_grad= 1.68504929649e-05\n",
      "i= 3000 LL= [-0.50569919685944531, -0.51852878596228247] max_grad= 1.40597113378e-05\n",
      "i= 4000 LL= [-0.49794377975990206, -0.51232383771829604] max_grad= 1.18956121357e-05\n",
      "i= 5000 LL= [-0.49185196588561353, -0.50764330218367604] max_grad= 1.02345071576e-05\n",
      "i= 6000 LL= [-0.48685119536255156, -0.50394283693107977] max_grad= 8.9455612659e-06\n",
      "i= 7000 LL= [-0.48261883459637056, -0.50091962767233877] max_grad= 7.92725184994e-06\n",
      "i= 8000 LL= [-0.47895596209487151, -0.49838906581996995] max_grad= 7.10712343527e-06\n",
      "i =  8450 entered zigzag LL =  -0.477456878462 max_grad= 6.78858814162e-06 decaying alpha to =  118.65234375\n",
      "i =  8470 entered zigzag LL =  -0.477411555635 max_grad= 6.77894522823e-06 decaying alpha to =  88.9892578125\n",
      "i =  8490 entered zigzag LL =  -0.477377613866 max_grad= 6.77173021835e-06 decaying alpha to =  66.7419433594\n",
      "i =  8500 entered zigzag LL =  -0.477365804981 max_grad= 6.76917053023e-06 decaying alpha to =  50.0564575195\n",
      "i =  8520 entered zigzag LL =  -0.477346740115 max_grad= 6.76512148021e-06 decaying alpha to =  37.5423431396\n",
      "i =  8530 entered zigzag LL =  -0.477340104652 max_grad= 6.7636841183e-06 decaying alpha to =  28.1567573547\n",
      "i =  8540 entered zigzag LL =  -0.477335129139 max_grad= 6.76260648198e-06 decaying alpha to =  21.1175680161\n",
      "i =  8550 entered zigzag LL =  -0.477331398115 max_grad= 6.76179847126e-06 decaying alpha to =  15.838176012\n",
      "i =  8560 entered zigzag LL =  -0.47732860019 max_grad= 6.76119258497e-06 decaying alpha to =  11.878632009\n",
      "i =  8570 entered zigzag LL =  -0.477326501939 max_grad= 6.76073823872e-06 decaying alpha to =  8.90897400677\n",
      "i =  8590 entered zigzag LL =  -0.477323112805 max_grad= 6.7600189768e-06 decaying alpha to =  6.68173050508\n",
      "i =  8600 entered zigzag LL =  -0.477321932762 max_grad= 6.75976348481e-06 decaying alpha to =  5.01129787881\n",
      "i =  8610 entered zigzag LL =  -0.477321047763 max_grad= 6.759571878e-06 decaying alpha to =  3.75847340911\n",
      "i =  8620 entered zigzag LL =  -0.477320384033 max_grad= 6.75942817974e-06 decaying alpha to =  2.81885505683\n",
      "i =  8630 entered zigzag LL =  -0.477319886247 max_grad= 6.7593204099e-06 decaying alpha to =  2.11414129262\n",
      "i =  8640 entered zigzag LL =  -0.477319512914 max_grad= 6.75923958468e-06 decaying alpha to =  1.58560596947\n",
      "i =  8650 entered zigzag LL =  -0.477319232917 max_grad= 6.75917896699e-06 decaying alpha to =  1.1892044771\n",
      "i =  8660 entered zigzag LL =  -0.477319022921 max_grad= 6.75913350441e-06 decaying alpha to =  0.891903357825\n",
      "i =  8670 entered zigzag LL =  -0.477318865426 max_grad= 6.75909940786e-06 decaying alpha to =  0.668927518369\n",
      "i =  8680 entered zigzag LL =  -0.477318747305 max_grad= 6.75907383566e-06 decaying alpha to =  0.501695638777\n",
      "i =  8690 entered zigzag LL =  -0.477318658714 max_grad= 6.75905465664e-06 decaying alpha to =  0.376271729083\n",
      "i =  8700 entered zigzag LL =  -0.477318592271 max_grad= 6.75904027244e-06 decaying alpha to =  0.282203796812\n",
      "i =  8710 entered zigzag LL =  -0.47731854244 max_grad= 6.75902948432e-06 decaying alpha to =  0.211652847609\n",
      "i =  8720 entered zigzag LL =  -0.477318505066 max_grad= 6.75902139326e-06 decaying alpha to =  0.158739635707\n",
      "i =  8730 entered zigzag LL =  -0.477318477035 max_grad= 6.75901532498e-06 decaying alpha to =  0.11905472678\n",
      "i =  8740 entered zigzag LL =  -0.477318456012 max_grad= 6.75901077377e-06 decaying alpha to =  0.089291045085\n",
      "i =  8750 entered zigzag LL =  -0.477318440245 max_grad= 6.75900736037e-06 decaying alpha to =  0.0669682838138\n",
      "i =  8760 entered zigzag LL =  -0.47731842842 max_grad= 6.75900480032e-06 decaying alpha to =  0.0502262128603\n",
      "i =  8770 entered zigzag LL =  -0.477318419551 max_grad= 6.75900288029e-06 decaying alpha to =  0.0376696596452\n",
      "i =  8780 entered zigzag LL =  -0.477318412899 max_grad= 6.75900144026e-06 decaying alpha to =  0.0282522447339\n",
      "i =  8790 entered zigzag LL =  -0.477318407911 max_grad= 6.75900036024e-06 decaying alpha to =  0.0211891835504\n",
      "i =  8800 entered zigzag LL =  -0.477318404169 max_grad= 6.75899955023e-06 decaying alpha to =  0.0158918876628\n",
      "i =  8810 entered zigzag LL =  -0.477318401363 max_grad= 6.75899894272e-06 decaying alpha to =  0.0119189157471\n",
      "i =  8820 entered zigzag LL =  -0.477318399258 max_grad= 6.75899848708e-06 decaying alpha to =  0.00893918681034\n",
      "i =  8830 entered zigzag LL =  -0.477318397679 max_grad= 6.75899814536e-06 decaying alpha to =  0.00670439010776\n",
      "i =  8840 entered zigzag LL =  -0.477318396496 max_grad= 6.75899788907e-06 decaying alpha to =  0.00502829258082\n",
      "i =  8850 entered zigzag LL =  -0.477318395608 max_grad= 6.75899769685e-06 decaying alpha to =  0.00377121943561\n",
      "i =  8860 entered zigzag LL =  -0.477318394942 max_grad= 6.75899755268e-06 decaying alpha to =  0.00282841457671\n",
      "i =  8870 entered zigzag LL =  -0.477318394443 max_grad= 6.75899744456e-06 decaying alpha to =  0.00212131093253\n",
      "i =  8880 entered zigzag LL =  -0.477318394068 max_grad= 6.75899736346e-06 decaying alpha to =  0.0015909831994\n",
      "i =  8890 entered zigzag LL =  -0.477318393787 max_grad= 6.75899730265e-06 decaying alpha to =  0.00119323739955\n",
      "i =  8900 entered zigzag LL =  -0.477318393576 max_grad= 6.75899725703e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 8901\n",
      "LL= [-0.47732492835973522, -0.49728904493041637]\n",
      "max_grad:  6.75899725703e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.0078125\n",
      "i =  0 LL decreased LL =  -1.10525265959 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21620938823 max_grad= 0.0438184844073 decaying alpha to =  281.25\n",
      "i =  860 entered zigzag LL =  -0.605346432406 max_grad= 0.020489823554 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.53657764656466522, -0.54582191036011662] max_grad= 0.00445795807693\n",
      "i =  1280 entered zigzag LL =  -0.530015148198 max_grad= 0.00567138046197 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.51587619584334821, -0.5272946530012691] max_grad= 1.69199909201e-05\n",
      "i= 3000 LL= [-0.50518616270330929, -0.51842133670747226] max_grad= 1.4140858481e-05\n",
      "i= 4000 LL= [-0.49732516404136751, -0.5121984112623168] max_grad= 1.1985578951e-05\n",
      "i= 5000 LL= [-0.49113117559164721, -0.50750162862047143] max_grad= 1.03316613993e-05\n",
      "i= 6000 LL= [-0.48603076337923073, -0.50378629232857208] max_grad= 9.04880559859e-06\n",
      "i= 7000 LL= [-0.48170074006123692, -0.50074937530297947] max_grad= 8.03576163918e-06\n",
      "i= 8000 LL= [-0.47794181106495803, -0.49820612232818301] max_grad= 7.22024243227e-06\n",
      "i =  8200 entered zigzag LL =  -0.477246175685 max_grad= 7.07608677627e-06 decaying alpha to =  118.65234375\n",
      "i =  8220 entered zigzag LL =  -0.477198114266 max_grad= 7.0660296423e-06 decaying alpha to =  88.9892578125\n",
      "i =  8240 entered zigzag LL =  -0.477162121757 max_grad= 7.05850513985e-06 decaying alpha to =  66.7419433594\n",
      "i =  8250 entered zigzag LL =  -0.477149599418 max_grad= 7.0558357418e-06 decaying alpha to =  50.0564575195\n",
      "i =  8270 entered zigzag LL =  -0.47712938275 max_grad= 7.05161324268e-06 decaying alpha to =  37.5423431396\n",
      "i =  8280 entered zigzag LL =  -0.477122346416 max_grad= 7.05011433677e-06 decaying alpha to =  28.1567573547\n",
      "i =  8290 entered zigzag LL =  -0.477117070319 max_grad= 7.04899056866e-06 decaying alpha to =  21.1175680161\n",
      "i =  8300 entered zigzag LL =  -0.477113113895 max_grad= 7.04814797383e-06 decaying alpha to =  15.838176012\n",
      "i =  8310 entered zigzag LL =  -0.477110146943 max_grad= 7.04751615774e-06 decaying alpha to =  11.878632009\n",
      "i =  8320 entered zigzag LL =  -0.477107921933 max_grad= 7.0470423688e-06 decaying alpha to =  8.90897400677\n",
      "i =  8340 entered zigzag LL =  -0.477104328057 max_grad= 7.04629233075e-06 decaying alpha to =  6.68173050508\n",
      "i =  8350 entered zigzag LL =  -0.477103076726 max_grad= 7.04602590755e-06 decaying alpha to =  5.01129787881\n",
      "i =  8360 entered zigzag LL =  -0.477102138264 max_grad= 7.04582610315e-06 decaying alpha to =  3.75847340911\n",
      "i =  8370 entered zigzag LL =  -0.477101434438 max_grad= 7.04567625717e-06 decaying alpha to =  2.81885505683\n",
      "i =  8380 entered zigzag LL =  -0.47710090658 max_grad= 7.0455638768e-06 decaying alpha to =  2.11414129262\n",
      "i =  8390 entered zigzag LL =  -0.477100510693 max_grad= 7.04547959383e-06 decaying alpha to =  1.58560596947\n",
      "i =  8400 entered zigzag LL =  -0.477100213781 max_grad= 7.04541638291e-06 decaying alpha to =  1.1892044771\n",
      "i =  8410 entered zigzag LL =  -0.4770999911 max_grad= 7.04536897545e-06 decaying alpha to =  0.891903357825\n",
      "i =  8420 entered zigzag LL =  -0.47709982409 max_grad= 7.04533342027e-06 decaying alpha to =  0.668927518369\n",
      "i =  8430 entered zigzag LL =  -0.477099698833 max_grad= 7.04530675411e-06 decaying alpha to =  0.501695638777\n",
      "i =  8440 entered zigzag LL =  -0.477099604891 max_grad= 7.04528675463e-06 decaying alpha to =  0.376271729083\n",
      "i =  8450 entered zigzag LL =  -0.477099534434 max_grad= 7.04527175509e-06 decaying alpha to =  0.282203796812\n",
      "i =  8460 entered zigzag LL =  -0.477099481592 max_grad= 7.04526050547e-06 decaying alpha to =  0.211652847609\n",
      "i =  8470 entered zigzag LL =  -0.47709944196 max_grad= 7.04525206828e-06 decaying alpha to =  0.158739635707\n",
      "i =  8480 entered zigzag LL =  -0.477099412236 max_grad= 7.04524574041e-06 decaying alpha to =  0.11905472678\n",
      "i =  8490 entered zigzag LL =  -0.477099389943 max_grad= 7.0452409945e-06 decaying alpha to =  0.089291045085\n",
      "i =  8500 entered zigzag LL =  -0.477099373224 max_grad= 7.04523743508e-06 decaying alpha to =  0.0669682838138\n",
      "i =  8510 entered zigzag LL =  -0.477099360684 max_grad= 7.04523476552e-06 decaying alpha to =  0.0502262128603\n",
      "i =  8520 entered zigzag LL =  -0.47709935128 max_grad= 7.04523276335e-06 decaying alpha to =  0.0376696596452\n",
      "i =  8530 entered zigzag LL =  -0.477099344226 max_grad= 7.04523126172e-06 decaying alpha to =  0.0282522447339\n",
      "i =  8540 entered zigzag LL =  -0.477099338936 max_grad= 7.0452301355e-06 decaying alpha to =  0.0211891835504\n",
      "i =  8550 entered zigzag LL =  -0.477099334969 max_grad= 7.04522929083e-06 decaying alpha to =  0.0158918876628\n",
      "i =  8560 entered zigzag LL =  -0.477099331992 max_grad= 7.04522865734e-06 decaying alpha to =  0.0119189157471\n",
      "i =  8570 entered zigzag LL =  -0.477099329761 max_grad= 7.04522818221e-06 decaying alpha to =  0.00893918681034\n",
      "i =  8580 entered zigzag LL =  -0.477099328087 max_grad= 7.04522782587e-06 decaying alpha to =  0.00670439010776\n",
      "i =  8590 entered zigzag LL =  -0.477099326832 max_grad= 7.04522755861e-06 decaying alpha to =  0.00502829258082\n",
      "i =  8600 entered zigzag LL =  -0.477099325891 max_grad= 7.04522735817e-06 decaying alpha to =  0.00377121943561\n",
      "i =  8610 entered zigzag LL =  -0.477099325184 max_grad= 7.04522720784e-06 decaying alpha to =  0.00282841457671\n",
      "i =  8620 entered zigzag LL =  -0.477099324654 max_grad= 7.04522709509e-06 decaying alpha to =  0.00212131093253\n",
      "i =  8630 entered zigzag LL =  -0.477099324257 max_grad= 7.04522701053e-06 decaying alpha to =  0.0015909831994\n",
      "i =  8640 entered zigzag LL =  -0.477099323959 max_grad= 7.0452269471e-06 decaying alpha to =  0.00119323739955\n",
      "i =  8650 entered zigzag LL =  -0.477099323736 max_grad= 7.04522689954e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 8651\n",
      "LL= [-0.47710625329151302, -0.49765303275604378]\n",
      "max_grad:  7.04522689954e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "beta =  0.00390625\n",
      "i =  0 LL decreased LL =  -1.10525181207 max_grad= 0.0214644046296 decaying alpha to =  375.0\n",
      "i= 0 LL= [-1.0986122886862781, -1.0986122886747014] max_grad= 10\n",
      "i =  10 LL decreased LL =  -1.21620421694 max_grad= 0.0438184015985 decaying alpha to =  281.25\n",
      "i =  860 entered zigzag LL =  -0.605195350009 max_grad= 0.0204860587894 decaying alpha to =  210.9375\n",
      "i= 1000 LL= [-0.53642951858603594, -0.54578095988623909] max_grad= 0.00444513426678\n",
      "i =  1280 entered zigzag LL =  -0.529844960386 max_grad= 0.00566211340304 decaying alpha to =  158.203125\n",
      "i= 2000 LL= [-0.51567428103955615, -0.52725119584397595] max_grad= 1.69548506767e-05\n",
      "i= 3000 LL= [-0.50492854756890027, -0.51836770421132372] max_grad= 1.41816153442e-05\n",
      "i= 4000 LL= [-0.49701419002193259, -0.51213584046645144] max_grad= 1.20308306e-05\n",
      "i= 5000 LL= [-0.4907684393718586, -0.5074309953258983] max_grad= 1.03806027277e-05\n",
      "i= 6000 LL= [-0.48561742641432537, -0.50370829547922524] max_grad= 9.1008973396e-06\n",
      "i= 7000 LL= [-0.48123768833681707, -0.50066460767371568] max_grad= 8.09059952341e-06\n",
      "i= 8000 LL= [-0.47742974516400677, -0.49811510361627076] max_grad= 7.27750538155e-06\n",
      "i =  8090 entered zigzag LL =  -0.477110216158 max_grad= 7.21213846356e-06 decaying alpha to =  118.65234375\n",
      "i =  8110 entered zigzag LL =  -0.477060825364 max_grad= 7.20189487318e-06 decaying alpha to =  88.9892578125\n",
      "i =  8130 entered zigzag LL =  -0.477023837322 max_grad= 7.19423107406e-06 decaying alpha to =  66.7419433594\n",
      "i =  8140 entered zigzag LL =  -0.477010968626 max_grad= 7.19151230133e-06 decaying alpha to =  50.0564575195\n",
      "i =  8160 entered zigzag LL =  -0.476990192783 max_grad= 7.18721174575e-06 decaying alpha to =  37.5423431396\n",
      "i =  8170 entered zigzag LL =  -0.476982961832 max_grad= 7.18568514466e-06 decaying alpha to =  28.1567573547\n",
      "i =  8180 entered zigzag LL =  -0.476977539804 max_grad= 7.18454061734e-06 decaying alpha to =  21.1175680161\n",
      "i =  8190 entered zigzag LL =  -0.47697347395 max_grad= 7.18368245995e-06 decaying alpha to =  15.838176012\n",
      "i =  8200 entered zigzag LL =  -0.476970424935 max_grad= 7.1830389758e-06 decaying alpha to =  11.878632009\n",
      "i =  8210 entered zigzag LL =  -0.476968138384 max_grad= 7.18255643798e-06 decaying alpha to =  8.90897400677\n",
      "i =  8230 entered zigzag LL =  -0.476964445107 max_grad= 7.18179255133e-06 decaying alpha to =  6.68173050508\n",
      "i =  8240 entered zigzag LL =  -0.476963159165 max_grad= 7.18152120934e-06 decaying alpha to =  5.01129787881\n",
      "i =  8250 entered zigzag LL =  -0.476962194747 max_grad= 7.18131771624e-06 decaying alpha to =  3.75847340911\n",
      "i =  8260 entered zigzag LL =  -0.476961471454 max_grad= 7.18116510395e-06 decaying alpha to =  2.81885505683\n",
      "i =  8270 entered zigzag LL =  -0.476960928996 max_grad= 7.18105064897e-06 decaying alpha to =  2.11414129262\n",
      "i =  8280 entered zigzag LL =  -0.476960522159 max_grad= 7.18096481012e-06 decaying alpha to =  1.58560596947\n",
      "i =  8290 entered zigzag LL =  -0.476960217035 max_grad= 7.18090043232e-06 decaying alpha to =  1.1892044771\n",
      "i =  8300 entered zigzag LL =  -0.476959988195 max_grad= 7.18085214972e-06 decaying alpha to =  0.891903357825\n",
      "i =  8310 entered zigzag LL =  -0.476959816565 max_grad= 7.1808159382e-06 decaying alpha to =  0.668927518369\n",
      "i =  8320 entered zigzag LL =  -0.476959687844 max_grad= 7.18078877979e-06 decaying alpha to =  0.501695638777\n",
      "i =  8330 entered zigzag LL =  -0.476959591303 max_grad= 7.18076841113e-06 decaying alpha to =  0.376271729083\n",
      "i =  8340 entered zigzag LL =  -0.476959518898 max_grad= 7.1807531347e-06 decaying alpha to =  0.282203796812\n",
      "i =  8350 entered zigzag LL =  -0.476959464594 max_grad= 7.18074167742e-06 decaying alpha to =  0.211652847609\n",
      "i =  8360 entered zigzag LL =  -0.476959423866 max_grad= 7.18073308449e-06 decaying alpha to =  0.158739635707\n",
      "i =  8370 entered zigzag LL =  -0.476959393321 max_grad= 7.1807266398e-06 decaying alpha to =  0.11905472678\n",
      "i =  8380 entered zigzag LL =  -0.476959370411 max_grad= 7.18072180629e-06 decaying alpha to =  0.089291045085\n",
      "i =  8390 entered zigzag LL =  -0.476959353229 max_grad= 7.18071818117e-06 decaying alpha to =  0.0669682838138\n",
      "i =  8400 entered zigzag LL =  -0.476959340343 max_grad= 7.18071546233e-06 decaying alpha to =  0.0502262128603\n",
      "i =  8410 entered zigzag LL =  -0.476959330678 max_grad= 7.18071342319e-06 decaying alpha to =  0.0376696596452\n",
      "i =  8420 entered zigzag LL =  -0.476959323429 max_grad= 7.18071189385e-06 decaying alpha to =  0.0282522447339\n",
      "i =  8430 entered zigzag LL =  -0.476959317992 max_grad= 7.18071074684e-06 decaying alpha to =  0.0211891835504\n",
      "i =  8440 entered zigzag LL =  -0.476959313915 max_grad= 7.18070988658e-06 decaying alpha to =  0.0158918876628\n",
      "i =  8450 entered zigzag LL =  -0.476959310857 max_grad= 7.18070924139e-06 decaying alpha to =  0.0119189157471\n",
      "i =  8460 entered zigzag LL =  -0.476959308564 max_grad= 7.18070875749e-06 decaying alpha to =  0.00893918681034\n",
      "i =  8470 entered zigzag LL =  -0.476959306844 max_grad= 7.18070839457e-06 decaying alpha to =  0.00670439010776\n",
      "i =  8480 entered zigzag LL =  -0.476959305554 max_grad= 7.18070812238e-06 decaying alpha to =  0.00502829258082\n",
      "i =  8490 entered zigzag LL =  -0.476959304586 max_grad= 7.18070791824e-06 decaying alpha to =  0.00377121943561\n",
      "i =  8500 entered zigzag LL =  -0.476959303861 max_grad= 7.18070776513e-06 decaying alpha to =  0.00282841457671\n",
      "i =  8510 entered zigzag LL =  -0.476959303316 max_grad= 7.1807076503e-06 decaying alpha to =  0.00212131093253\n",
      "i =  8520 entered zigzag LL =  -0.476959302908 max_grad= 7.18070756418e-06 decaying alpha to =  0.0015909831994\n",
      "i =  8530 entered zigzag LL =  -0.476959302601 max_grad= 7.18070749959e-06 decaying alpha to =  0.00119323739955\n",
      "i =  8540 entered zigzag LL =  -0.476959302372 max_grad= 7.18070745114e-06 decaying alpha to =  0.000894928049662\n",
      "\n",
      "training halted: Nitermax reached or alpha decayed to insanely small value\n",
      "training iterations: 8541\n",
      "LL= [-0.47696642359021435, -0.49781128342162223]\n",
      "max_grad:  7.18070745114e-06 \n",
      "\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "## Note to self: the optimization problem is concave so the training log likelihood should always increase,\n",
    "## assuming the learning rate is small enough. If we get too close to the global max, the lambda's will be really \n",
    "## small and the change in log likelihood will be dominated by noise, and so may increase. This should indicate\n",
    "## end of training, and not a problem with the code, especially since I checked the gradients above.\n",
    "\n",
    "## ideas for faster convergence\n",
    "## 1. momentum method\n",
    "## 2. line search\n",
    "\n",
    "alpha_init = 500 ## initial learning rate\n",
    "max_grad_TOL = 1e-6 ## when the maximum gradient is smaller than this, consider it converged\n",
    "Nitermax = 1e6\n",
    "m_eval = 10 ## evaluate only every m_eval iterations\n",
    "alpha_min = 1e-3  ## if alpha decays past this, something's gone wrong\n",
    "zigzag_TOL = 1e-3\n",
    "## initialize some lists\n",
    "lambda_list = []\n",
    "LL = []\n",
    "LL_cv = []\n",
    "\n",
    "## scan over values of the regulator variable\n",
    "for j in range(len(beta_list)):\n",
    "  beta = beta_list[j]\n",
    "  ## create the vector of parameters to be learned (initialized to zero)\n",
    "  lambda_neg = np.zeros(max_rank)\n",
    "  lambda_neutral = np.zeros(max_rank)\n",
    "  lambda_pos = np.zeros(max_rank)\n",
    "  ## the initial LL and accuracy for training set and cv set\n",
    "  LL_history = [loglike(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)]\n",
    "  LL_cv_history = [loglike(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos, 0)]\n",
    "  max_grad_history = [10]\n",
    "  \n",
    "  ## gradient ascent\n",
    "  cond = True\n",
    "  i = 0\n",
    "  alpha = alpha_init ## learning rate\n",
    "  print 'beta = ', beta\n",
    "  while cond:    \n",
    "    ## compute the gradients\n",
    "    [dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos] = gradient(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)\n",
    "    ## record the current values\n",
    "    if i % m_eval == 0:\n",
    "      lambda_neg_old = lambda_neg\n",
    "      lambda_neutral_old = lambda_neutral\n",
    "      lambda_pos_old = lambda_pos\n",
    "    ## advance the parameters\n",
    "    lambda_neg = lambda_neg + alpha*dLLdlambda_neg\n",
    "    lambda_neutral = lambda_neutral + alpha*dLLdlambda_neutral\n",
    "    lambda_pos = lambda_pos + alpha*dLLdlambda_pos\n",
    "    \n",
    "    if i % m_eval == 0:\n",
    "      ## compute some diagnostics\n",
    "      LL_current = loglike(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)\n",
    "      LL_cv_current = loglike(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos, 0)\n",
    "      max_grad = np.max(np.abs(np.concatenate((dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos), axis=0)))\n",
    "\n",
    "      ## decay learning rate if log likelihood decreases before convergence or if we enter a zigzag loop\n",
    "      zigzag = np.abs((max_grad - max_grad_history[-1])/max_grad) < zigzag_TOL #not sure if this is the best way to detect zigzags\n",
    "      LL_decrease = LL_current < LL_history[-1]\n",
    "      alpha_decay = (LL_decrease or zigzag) and (max_grad_history[-1] > max_grad_TOL)\n",
    "      if alpha_decay:\n",
    "        ## reset the lambdas\n",
    "        lambda_neg = lambda_neg_old\n",
    "        lambda_neutral = lambda_neutral_old\n",
    "        lambda_pos = lambda_pos_old\n",
    "        ## decay alpha\n",
    "        alpha = 0.75*alpha\n",
    "        if LL_decrease:\n",
    "          print 'i = ', i, 'LL decreased', 'LL = ', LL_current, 'max_grad=', max_grad, 'decaying alpha to = ', alpha\n",
    "        if zigzag:\n",
    "          print 'i = ', i, 'entered zigzag', 'LL = ', LL_current, 'max_grad=', max_grad, 'decaying alpha to = ', alpha\n",
    "  \n",
    "      ## record-keeping\n",
    "      if not alpha_decay:\n",
    "        LL_history.append(LL_current)\n",
    "        LL_cv_history.append(LL_cv_current)\n",
    "        max_grad_history.append(max_grad)    \n",
    "        \n",
    "      ## print out every 1000 iterations\n",
    "      if i % 1000 == 0:\n",
    "        print 'i=', i, 'LL=', [LL_history[-1], LL_cv_history[-1]], 'max_grad=', max_grad_history[-1]\n",
    "      \n",
    "    ## advance the gradient ascent iterations\n",
    "    i += 1\n",
    "    \n",
    "    ## end the training if it has converged\n",
    "    if max_grad < max_grad_TOL:\n",
    "      ## compute the derivatives at the final stopping point\n",
    "      [dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos] = gradient(df, feature, lambda_neg, lambda_neutral, lambda_pos, beta)\n",
    "      max_grad = np.max(np.abs(np.concatenate((dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos), axis=0)))\n",
    "      print '\\ntraining halted: gradients converged'\n",
    "      print 'training iterations:', i, 'LL=', [LL_history[-1], LL_cv_history[-1]]\n",
    "      print 'max_grad: ', max_grad, '\\n'\n",
    "      print '------------------------------------------------------------------------'\n",
    "      cond = False\n",
    "    \n",
    "    ## end the training if something has gone wrong\n",
    "    if i == Nitermax or alpha < alpha_min:\n",
    "      print '\\ntraining halted: Nitermax reached or alpha decayed to insanely small value'\n",
    "      print 'training iterations:', i\n",
    "      print 'LL=', [LL_history[-1], LL_cv_history[-1]]\n",
    "      print 'max_grad: ', max_grad, '\\n'\n",
    "      print '------------------------------------------------------------------------'\n",
    "      cond = False\n",
    "    \n",
    "  ## Save the ME parameters at the end of every successful iteration\n",
    "  lambda_list.append([lambda_neg, lambda_neutral, lambda_pos])\n",
    "  LL.append(LL_history)\n",
    "  LL_cv.append(LL_cv_history)\n",
    "  np.save(path + 'lambda_list', lambda_list)\n",
    "  np.save(path + 'beta_list', beta_list[:j+1])\n",
    "  np.save(path + 'LL_cv', LL_cv)\n",
    "  np.save(path + 'LL', LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAFiCAYAAAAEBkVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe77fea0ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Training plots\n",
    "\n",
    "## Find the best value of beta by utilizing the CV set\n",
    "jbest = [ LL_cv[i][-1] for i in range(len(LL_cv)) ].index(max([ LL_cv[i][-1] for i in range(len(LL_cv)) ]))\n",
    "lambda_neg = lambda_list[jbest][0]\n",
    "lambda_neutral = lambda_list[jbest][1]\n",
    "lambda_pos = lambda_list[jbest][2]\n",
    "LL_history = LL[jbest]\n",
    "LL_cv_history = LL_cv[jbest]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "## subplot 1: the training history\n",
    "sub1 = fig.add_subplot(121)\n",
    "sub1.set_title('Training History ' + r'$(\\beta=\\beta^*)$', fontsize=20)\n",
    "sub1.plot(m_eval*np.array(range(len(LL_history))), LL_history, c='black')\n",
    "sub1.plot(m_eval*np.array(range(len(LL_history))), LL_cv_history, c='black', linestyle='--')\n",
    "sub1.set_ylabel('avg. log likelihood', fontsize=20)\n",
    "sub1.set_xlabel('gradient ascent iterations', fontsize=20)\n",
    "sub1.set_xlim([0, m_eval*len(LL_history)])\n",
    "## subplot 2: cv analysis\n",
    "sub2 = fig.add_subplot(122)\n",
    "sub2.set_title('L2 regularization', fontsize=20)\n",
    "sub2.plot(np.log2(beta_list), [ LL_cv[i][-1] for i in range(len(LL_cv))], color='black')\n",
    "sub2.axvline(np.log2(beta_list)[jbest], linestyle=':', color='red')\n",
    "sub2.set_ylabel('avg. log likelihood', fontsize=20)\n",
    "sub2.set_xlabel(r'$\\log_2$' + r'$\\beta$', fontsize=20)\n",
    "sub2.set_xlim([np.log2(beta_list)[-1], np.log2(beta_list)[0]])\n",
    "## finish up\n",
    "plt.tight_layout()\n",
    "if small:\n",
    "  plt.savefig('figures/small_MaxEnt_gradient_ascent.png')\n",
    "else:\n",
    "  plt.savefig('figures/MaxEnt_gradient_ascent.png')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance:\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "| data set | accuracy | (-) accuracy | (0) accuracy | (+) accuracy |\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "|    CV    |   77.8   |     68.7     |    93.85     |    70.87     |\n",
      "|   S140   |  55.73   |    50.28     |    64.75     |    54.14     |\n",
      "|    SA    |  51.94   |    33.77     |    57.82     |    46.31     |\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "MaxEnt Performance:\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "| data set | accuracy | (-) accuracy | (0) accuracy | (+) accuracy |\n",
      "+----------+----------+--------------+--------------+--------------+\n",
      "|    CV    |   80.1   |    77.05     |    91.45     |     71.8     |\n",
      "|   S140   |  62.78   |    58.76     |    58.27     |    70.17     |\n",
      "|    SA    |  48.25   |    49.01     |    46.91     |    53.44     |\n",
      "+----------+----------+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "## Classification error\n",
    "\n",
    "ME_pred_cv = ME_predictions(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos)\n",
    "ME_pred_sa = ME_predictions(df_sa, feature_sa, lambda_neg, lambda_neutral, lambda_pos)\n",
    "ME_pred_s140 = ME_predictions(df_s140, feature_s140, lambda_neg, lambda_neutral, lambda_pos)\n",
    "## training CV set\n",
    "ME_accuracy_cv_neg = 100.0*sum([ME_pred_cv[i] for i in range(len(df_cv)) if list(df_cv['sentiment'] == -1)[i] ] == df_cv[df_cv['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == -1])\n",
    "ME_accuracy_cv_neutral = 100.0*sum([ME_pred_cv[i] for i in range(len(df_cv)) if list(df_cv['sentiment'] == 0)[i] ] == df_cv[df_cv['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == 0])\n",
    "ME_accuracy_cv_pos = 100.0*sum([ME_pred_cv[i] for i in range(len(df_cv)) if list(df_cv['sentiment'] == 1)[i] ] == df_cv[df_cv['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_cv[df_cv['sentiment'] == 1])\n",
    "## S140 CV set\n",
    "ME_accuracy_s140_neg = 100.0*sum([ME_pred_s140[i] for i in range(len(df_s140)) if list(df_s140['sentiment'] == -1)[i] ] == df_s140[df_s140['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == -1])\n",
    "ME_accuracy_s140_neutral = 100.0*sum([ME_pred_s140[i] for i in range(len(df_s140)) if list(df_s140['sentiment'] == 0)[i] ] == df_s140[df_s140['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == 0])\n",
    "ME_accuracy_s140_pos = 100.0*sum([ME_pred_s140[i] for i in range(len(df_s140)) if list(df_s140['sentiment'] == 1)[i] ] == df_s140[df_s140['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_s140[df_s140['sentiment'] == 1])\n",
    "## SA CV set\n",
    "ME_accuracy_sa_neg = 100.0*sum([ME_pred_sa[i] for i in range(len(df_sa)) if list(df_sa['sentiment'] == -1)[i] ] == df_sa[df_sa['sentiment'] == -1]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == -1])\n",
    "ME_accuracy_sa_neutral = 100.0*sum([ME_pred_sa[i] for i in range(len(df_sa)) if list(df_sa['sentiment'] == 0)[i] ] == df_sa[df_sa['sentiment'] == 0]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == 0])\n",
    "ME_accuracy_sa_pos = 100.0*sum([ME_pred_sa[i] for i in range(len(df_sa)) if list(df_sa['sentiment'] == 1)[i] ] == df_sa[df_sa['sentiment'] == 1]['sentiment'])\\\n",
    "                    /len(df_sa[df_sa['sentiment'] == 1])\n",
    "\n",
    "## examine performance\n",
    "print 'Naive Bayes Performance:'\n",
    "pt = PrettyTable(field_names=['data set', 'accuracy', '(-) accuracy', '(0) accuracy', '(+) accuracy']) \n",
    "[ pt.add_row(['CV', round(NB_accuracy_cv, 2), round(NB_accuracy_cv_neg, 2), \\\n",
    "              round(NB_accuracy_cv_neutral, 2), round(NB_accuracy_cv_pos, 2) ])]\n",
    "\n",
    "[ pt.add_row(['S140', round(NB_accuracy_s140, 2), round(NB_accuracy_s140_neg, 2), \\\n",
    "              round(NB_accuracy_s140_neutral, 2), round(NB_accuracy_s140_pos, 2) ])]\n",
    "\n",
    "[ pt.add_row(['SA', round(NB_accuracy_sa, 2), round(NB_accuracy_sa_neg, 2), \\\n",
    "              round(NB_accuracy_sa_neutral, 2), round(NB_accuracy_sa_pos, 2) ])]\n",
    "\n",
    "#pt.align['Word'], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "print pt\n",
    "\n",
    "print 'MaxEnt Performance:'\n",
    "pt = PrettyTable(field_names=['data set', 'accuracy', '(-) accuracy', '(0) accuracy', '(+) accuracy']) \n",
    "[ pt.add_row(['CV', round(100.0*sum(ME_pred_cv == df_cv['sentiment'])/len(df_cv),2), \\\n",
    "                                   round(ME_accuracy_cv_neg,2), round(ME_accuracy_cv_neutral,2), round(ME_accuracy_cv_pos,2) ])]\n",
    "\n",
    "[ pt.add_row(['S140', round(100.0*sum(ME_pred_s140 == df_s140['sentiment'])/len(df_s140),2), \\\n",
    "                               round(ME_accuracy_s140_neg,2), round(ME_accuracy_s140_neutral,2), round(ME_accuracy_s140_pos,2) ])]\n",
    "\n",
    "[ pt.add_row(['SA', round(100.0*sum(ME_pred_sa == df_sa['sentiment'])/len(df_sa),2), \\\n",
    "                             round(ME_accuracy_sa_neg,2), round(ME_accuracy_sa_neutral,2), round(ME_accuracy_sa_pos,2) ])]\n",
    "\n",
    "#pt.align['Word'], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+---------+\n",
      "| data set | avg. sentiment | NB pred | ME pred |\n",
      "+----------+----------------+---------+---------+\n",
      "|    CV    |      0.0       |   0.04  |  -0.012 |\n",
      "|   S140   |     0.008      |   0.0   |  0.091  |\n",
      "|    SA    |     -0.023     |   0.26  |  0.207  |\n",
      "+----------+----------------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "## examine sentiment\n",
    "pt = PrettyTable(field_names=['data set', 'avg. sentiment', 'NB pred', 'ME pred']) \n",
    "[ pt.add_row(['CV', round(1.0*sum(df_cv['sentiment'])/len(df_cv),3), \\\n",
    "              round(1.0*sum(df_cv['NB predictions'])/len(df_cv),3), round(1.0*sum(ME_pred_cv)/len(df_cv),3) ]) ]\n",
    "[ pt.add_row(['S140', round(1.0*sum(df_s140['sentiment'])/len(df_s140),3), \\\n",
    "              round(1.0*sum(df_s140['NB predictions'])/len(df_s140),3), round(1.0*sum(ME_pred_s140)/len(df_s140),3) ]) ]\n",
    "[ pt.add_row(['SA', round(1.0*sum(df_sa['sentiment'])/len(df_sa),3), \\\n",
    "              round(1.0*sum(df_sa['NB predictions'])/len(df_sa),3), round(1.0*sum(ME_pred_sa)/len(df_sa),3) ]) ]\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23622309  0.04409779  0.05301246]\n",
      " [ 0.01783383  0.31283004  0.00266947]\n",
      " [ 0.07035277  0.03398522  0.22899534]]\n",
      "\n",
      "\n",
      "[[ 0.1971831   0.11871227  0.04828974]\n",
      " [ 0.04225352  0.18108652  0.05633803]\n",
      " [ 0.04426559  0.13279678  0.17907445]]\n",
      "\n",
      "\n",
      "[[ 0.06935976  0.07202744  0.00838415]\n",
      " [ 0.24695122  0.39176829  0.03887195]\n",
      " [ 0.04954268  0.06478659  0.05830793]]\n"
     ]
    }
   ],
   "source": [
    "## Work out the confusion matrices for the NB predictions\n",
    "M_NB_cv_pp = 1.0*sum(((df_cv['sentiment'] == 1) & (df_cv['NB predictions'] == 1)))/len(df_cv)\n",
    "M_NB_cv_p0 = 1.0*sum(((df_cv['sentiment'] == 1) & (df_cv['NB predictions'] == 0)))/len(df_cv)\n",
    "M_NB_cv_pn = 1.0*sum(((df_cv['sentiment'] == 1) & (df_cv['NB predictions'] == -1)))/len(df_cv)\n",
    "M_NB_cv_0p = 1.0*sum(((df_cv['sentiment'] == 0) & (df_cv['NB predictions'] == 1)))/len(df_cv)\n",
    "M_NB_cv_00 = 1.0*sum(((df_cv['sentiment'] == 0) & (df_cv['NB predictions'] == 0)))/len(df_cv)\n",
    "M_NB_cv_0n = 1.0*sum(((df_cv['sentiment'] == 0) & (df_cv['NB predictions'] == -1)))/len(df_cv)\n",
    "M_NB_cv_np = 1.0*sum(((df_cv['sentiment'] == -1) & (df_cv['NB predictions'] == 1)))/len(df_cv)\n",
    "M_NB_cv_n0 = 1.0*sum(((df_cv['sentiment'] == -1) & (df_cv['NB predictions'] == 0)))/len(df_cv)\n",
    "M_NB_cv_nn = 1.0*sum(((df_cv['sentiment'] == -1) & (df_cv['NB predictions'] == -1)))/len(df_cv)\n",
    "\n",
    "M_NB_s140_pp = 1.0*sum(((df_s140['sentiment'] == 1) & (df_s140['NB predictions'] == 1)))/len(df_s140)\n",
    "M_NB_s140_p0 = 1.0*sum(((df_s140['sentiment'] == 1) & (df_s140['NB predictions'] == 0)))/len(df_s140)\n",
    "M_NB_s140_pn = 1.0*sum(((df_s140['sentiment'] == 1) & (df_s140['NB predictions'] == -1)))/len(df_s140)\n",
    "M_NB_s140_0p = 1.0*sum(((df_s140['sentiment'] == 0) & (df_s140['NB predictions'] == 1)))/len(df_s140)\n",
    "M_NB_s140_00 = 1.0*sum(((df_s140['sentiment'] == 0) & (df_s140['NB predictions'] == 0)))/len(df_s140)\n",
    "M_NB_s140_0n = 1.0*sum(((df_s140['sentiment'] == 0) & (df_s140['NB predictions'] == -1)))/len(df_s140)\n",
    "M_NB_s140_np = 1.0*sum(((df_s140['sentiment'] == -1) & (df_s140['NB predictions'] == 1)))/len(df_s140)\n",
    "M_NB_s140_n0 = 1.0*sum(((df_s140['sentiment'] == -1) & (df_s140['NB predictions'] == 0)))/len(df_s140)\n",
    "M_NB_s140_nn = 1.0*sum(((df_s140['sentiment'] == -1) & (df_s140['NB predictions'] == -1)))/len(df_s140)\n",
    "\n",
    "M_NB_sa_pp = 1.0*sum(((df_sa['sentiment'] == 1) & (df_sa['NB predictions'] == 1)))/len(df_sa)\n",
    "M_NB_sa_p0 = 1.0*sum(((df_sa['sentiment'] == 1) & (df_sa['NB predictions'] == 0)))/len(df_sa)\n",
    "M_NB_sa_pn = 1.0*sum(((df_sa['sentiment'] == 1) & (df_sa['NB predictions'] == -1)))/len(df_sa)\n",
    "M_NB_sa_0p = 1.0*sum(((df_sa['sentiment'] == 0) & (df_sa['NB predictions'] == 1)))/len(df_sa)\n",
    "M_NB_sa_00 = 1.0*sum(((df_sa['sentiment'] == 0) & (df_sa['NB predictions'] == 0)))/len(df_sa)\n",
    "M_NB_sa_0n = 1.0*sum(((df_sa['sentiment'] == 0) & (df_sa['NB predictions'] == -1)))/len(df_sa)\n",
    "M_NB_sa_np = 1.0*sum(((df_sa['sentiment'] == -1) & (df_sa['NB predictions'] == 1)))/len(df_sa)\n",
    "M_NB_sa_n0 = 1.0*sum(((df_sa['sentiment'] == -1) & (df_sa['NB predictions'] == 0)))/len(df_sa)\n",
    "M_NB_sa_nn = 1.0*sum(((df_sa['sentiment'] == -1) & (df_sa['NB predictions'] == -1)))/len(df_sa)\n",
    "\n",
    "M_NB_cv = np.asarray([[M_NB_cv_pp, M_NB_cv_p0, M_NB_cv_pn], \\\n",
    "                      [M_NB_cv_0p, M_NB_cv_00, M_NB_cv_0n], [M_NB_cv_np, M_NB_cv_n0, M_NB_cv_nn]])\n",
    "\n",
    "M_NB_s140 = np.asarray([[M_NB_s140_pp, M_NB_s140_p0, M_NB_s140_pn], \\\n",
    "                      [M_NB_s140_0p, M_NB_s140_00, M_NB_s140_0n], [M_NB_s140_np, M_NB_s140_n0, M_NB_s140_nn]])\n",
    "\n",
    "M_NB_sa = np.asarray([[M_NB_sa_pp, M_NB_sa_p0, M_NB_sa_pn], \\\n",
    "                      [M_NB_sa_0p, M_NB_sa_00, M_NB_sa_0n], [M_NB_sa_np, M_NB_sa_n0, M_NB_sa_nn]])\n",
    "\n",
    "print M_NB_cv\n",
    "print '\\n'\n",
    "print M_NB_s140\n",
    "print '\\n'\n",
    "print M_NB_sa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2, Gavin",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
