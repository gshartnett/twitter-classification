{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "## load the processed training-set tweets\n",
    "df = pd.read_pickle('tweet_data/training_data/training_data_processed.pkl')\n",
    "## load the processed cv-set tweets\n",
    "df_sa_cv = pd.read_pickle('tweet_data/cv_data/sanders_analytics/sanders_analytics_cv_data_processed.pkl')\n",
    "df_s140_cv = pd.read_pickle('tweet_data/cv_data/sentiment140/sentiment140_cv_data_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## drop all neutral tweets\n",
    "neutral_drop = (df['sentiment']==0)\n",
    "droplist = list(neutral_drop)\n",
    "not_droplist = [not i for i in droplist]\n",
    "df = df[list(not_droplist)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## drop all neutral tweets\n",
    "neutral_drop = (df_sa_cv['sentiment']==0)\n",
    "droplist = list(neutral_drop)\n",
    "not_droplist = [not i for i in droplist]\n",
    "df_sa_cv = df_sa_cv[list(not_droplist)]\n",
    "df_sa_cv = df_sa_cv.reset_index(drop=True)\n",
    "\n",
    "## drop all neutral tweets\n",
    "neutral_drop = (df_s140_cv['sentiment']==0)\n",
    "droplist = list(neutral_drop)\n",
    "not_droplist = [not i for i in droplist]\n",
    "df_s140_cv = df_s140_cv[list(not_droplist)]\n",
    "df_s140_cv = df_s140_cv.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data set size: 423867\n",
      "cv data set size: 47096\n",
      "sanders anlaytics cv data set size: 846\n",
      "sentiment 140 cv data set size: 358\n"
     ]
    }
   ],
   "source": [
    "## split off a separate cv set from the training data (we already shuffled the data)\n",
    "cvratio = .1 # fraction to split off\n",
    "df_cv = df.copy(deep=True)\n",
    "df_cv = df_cv.iloc[:int(math.floor(cvratio*len(df)))]\n",
    "df_cv = df_cv.reset_index(drop=True)\n",
    "df = df.iloc[int(math.floor(cvratio*len(df))):]\n",
    "df = df.reset_index(drop=True)\n",
    "print 'training data set size:', len(df)\n",
    "print 'cv data set size:', + len(df_cv)\n",
    "print 'sanders anlaytics cv data set size:', len(df_sa_cv)\n",
    "print 'sentiment 140 cv data set size:', len(df_s140_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 93.9903151697% of emoticons\n"
     ]
    }
   ],
   "source": [
    "## throw away some fraction (to be treated as a hyperparameter) of the emoticons\n",
    "## probably should treat +/- differently, for now let's treat as the same\n",
    "pThreshold = 0.06 #fraction to keep\n",
    "emoticoncounter = 0\n",
    "droppedcounter = 0\n",
    "for i in range(len(df)):\n",
    "  for w in df['words'].iloc[i]:\n",
    "    if w == 'posemoticontoken' or w == 'negemoticontoken':\n",
    "      emoticoncounter += 1\n",
    "      if random.random() > pThreshold:\n",
    "        df['words'].iloc[i].remove(w)\n",
    "        droppedcounter += 1\n",
    "print 'dropped ' + str(100*float(droppedcounter)/emoticoncounter) + '% of emoticons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## build the bag of words\n",
    "words_tot_pos = [item for sublist in df['words'][df['sentiment'] == 1] for item in sublist]\n",
    "words_tot_neg = [item for sublist in df['words'][df['sentiment'] == -1] for item in sublist]\n",
    "words_tot = words_tot_pos + words_tot_neg\n",
    "counter_pos = Counter(words_tot_pos)\n",
    "counter_neg = Counter(words_tot_neg)\n",
    "counter = Counter(words_tot)\n",
    "\n",
    "## priors\n",
    "prior_pos = float(sum(df['sentiment']==1))/len(df)\n",
    "prior_neg = float(sum(df['sentiment']==-1))/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## conditional probabilities of word given class (with Laplace smoothing parameter alpha)\n",
    "def p_word_given_pos(word):\n",
    "  return float(counter_pos[word] + alpha)/(len(words_tot_pos) + alpha*len(words_tot))\n",
    "def p_word_given_neg(word):\n",
    "  return float(counter_neg[word] + alpha)/(len(words_tot_neg) + alpha*len(words_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(word_list):\n",
    "  p_pos = math.log(prior_pos) + sum([math.log(p_word_given_pos(i)) for i in word_list])\n",
    "  p_neg = math.log(prior_neg) + sum([math.log(p_word_given_neg(i)) for i in word_list])\n",
    "  return 2*([p_neg, p_pos].index(max([p_pos, p_neg])))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctly classified: 99.9617801936 %\n",
      "correctly classified: 73.1678486998 %\n",
      "correctly classified: 74.3016759777 %\n"
     ]
    }
   ],
   "source": [
    "## evaluate against cv sets\n",
    "alpha = 1 # Laplace smoothing parameter\n",
    "df_cv['predictions'] = df_cv['words'].map(predict)\n",
    "df_sa_cv['predictions'] = df_sa_cv['words'].map(predict)\n",
    "df_s140_cv['predictions'] = df_s140_cv['words'].map(predict)\n",
    "\n",
    "print 'correctly classified:', 100*float(sum(df_cv['predictions'] == df_cv['sentiment']))/len(df_cv), '%'\n",
    "print 'correctly classified:', 100*float(sum(df_sa_cv['predictions'] == df_sa_cv['sentiment']))/len(df_sa_cv), '%'\n",
    "print 'correctly classified:', 100*float(sum(df_s140_cv['predictions'] == df_s140_cv['sentiment']))/len(df_s140_cv), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=9\n",
    "print df_s140_cv['words'].iloc[i]\n",
    "print df_s140_cv['sentiment'].iloc[i]\n",
    "print predict(df_s140_cv['words'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_s140_cv['predictions'] == df_s140_cv['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2, Gavin",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
