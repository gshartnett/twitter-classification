{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "## load the dataframe of (mostly) raw tweets\n",
    "df = pd.read_pickle('tweet_data/tweets.pkl')\n",
    "\n",
    "###TODO: \n",
    "#deal with unicode, #print df['text'][jj].encode('raw_unicode_escape')\n",
    "#maybe expand list of emoticons\n",
    "#make sure they are all english!\n",
    "#maybe exclude :P's from positive list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of tweets: 846811\n",
      "  number of neutral tweets: 334728, 39.5280647039%\n",
      "  number of positive tweets: 255974, 30.2279965659%\n",
      "  number of negative tweets: 256109, 30.2439387301%\n"
     ]
    }
   ],
   "source": [
    "## print out some basic initial statistics\n",
    "print 'initial number of tweets: ' + str(len(df))\n",
    "print '  number of neutral tweets: ' + str(sum(df['sentiment']==0)) + ', ' + str(float(sum(df['sentiment']==0))/len(df)*100) + '%'\n",
    "print '  number of positive tweets: ' + str(sum(df['sentiment']==1)) + ', ' + str(float(sum(df['sentiment']==1))/len(df)*100) + '%'\n",
    "print '  number of negative tweets: ' + str(sum(df['sentiment']==-1)) + ', ' + str(float(sum(df['sentiment']==-1))/len(df)*100) + '%'\n",
    "\n",
    "if sum(df['sentiment']==-1) + sum(df['sentiment']==0) + sum(df['sentiment']==1) != len(df):\n",
    "  print 'Not all tweets classified as 1,0,-1!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates deleted: 6219\n",
      "current number of tweets: 840592\n"
     ]
    }
   ],
   "source": [
    "## delete the duplicates\n",
    "Noriginal = len(df)\n",
    "df = df.drop_duplicates(subset='id', keep='last')\n",
    "print 'duplicates deleted: ' + str(Noriginal - len(df))\n",
    "df = df.reset_index(drop=True)\n",
    "print 'current number of tweets: ' + str(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_emoticons[0].encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-33cb36b202d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/gavin/anaconda2/lib/python2.7/site-packages/pandas/core/strings.pyc\u001b[0m in \u001b[0;36mcontains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m         result = str_contains(self._data, pat, case=case, flags=flags, na=na,\n\u001b[1;32m-> 1458\u001b[1;33m                               regex=regex)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gavin/anaconda2/lib/python2.7/site-packages/pandas/core/strings.pyc\u001b[0m in \u001b[0;36mstr_contains\u001b[1;34m(arr, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gavin/anaconda2/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gavin/anaconda2/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(*key)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;31m# invalid expression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbypass_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unbalanced parenthesis"
     ]
    }
   ],
   "source": [
    "df['text'].str.contains(str(':)'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-English tweets deleted: 4910\n",
      "current number of tweets: 835682\n"
     ]
    }
   ],
   "source": [
    "## verify that all tweets are in English\n",
    "## most of these seem to actually be English...\n",
    "lang_drop = (df['lang']!='en')\n",
    "print 'non-English tweets deleted: ' + str(sum(lang_drop))\n",
    "df = df[list([not i for i in lang_drop])]\n",
    "df = df.reset_index(drop=True)\n",
    "print 'current number of tweets: ' + str(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets to be dropped: 19841, 2.44212853269%\n",
      "  neutral tweets to be dropped: 1411, 0.437263123317%\n",
      "  positive tweets to be dropped: 16813, 7.04672811021%\n",
      "  negative tweets to be dropped: 3028, 1.20558198794%\n"
     ]
    }
   ],
   "source": [
    "## In the case of the pos/neg tweets, the twitter search returned results \"similiar\" \n",
    "## to the query keywords ':)', ':('. Here, refine the definition of positive/neutral/negative such that if\n",
    "#pos_emoticons = [u':)', u':-)', u':D', u': )', u'=)']\n",
    "#neg_emoticons = [u':(', u':-(', u': (']\n",
    "pos_emoticons = [':\\)', ':-\\)', ':D', ': \\)', '=\\)']\n",
    "neg_emoticons = [':\\(', ':-\\(', ': \\(']\n",
    "emoticons = pos_emoticons + neg_emoticons\n",
    "## Then positive tweets will contain at least 1 of pos_emoticons and none of the neg_emoticons, and vice versa.\n",
    "## Also, delete any neutral tweets containing any of these emoticons\n",
    "\n",
    "## first, drop the tweets containing the wrong emoticons\n",
    "neutral_drop = (df['sentiment']==0) & (df['text'].str.contains(emoticons[0]))\n",
    "for i in range(len(emoticons)-1):\n",
    "  dum = (df['sentiment']==0) & (df['text'].str.contains(emoticons[i+1]))\n",
    "  neutral_drop = neutral_drop | dum\n",
    "\n",
    "pos_drop = (df['sentiment']==1) & (df['text'].str.contains(neg_emoticons[0]))\n",
    "for i in range(len(neg_emoticons)-1):\n",
    "  dum = (df['sentiment']==1) & (df['text'].str.contains(neg_emoticons[i+1]))\n",
    "  pos_drop = pos_drop | dum\n",
    "  \n",
    "neg_drop = (df['sentiment']==-1) & (df['text'].str.contains(pos_emoticons[0]))\n",
    "for i in range(len(pos_emoticons)-1):\n",
    "  dum = (df['sentiment']==-1) & (df['text'].str.contains(pos_emoticons[i+1]))\n",
    "  neg_drop = neg_drop | dum\n",
    "\n",
    "droplist = list(neutral_drop | pos_drop | neg_drop)\n",
    "not_droplist = [not i for i in droplist]\n",
    "df = df[list(not_droplist)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## next, drop any positive/negative tweets not containing at least one of the right emoticons\n",
    "pos_drop = (df['sentiment']==1) & (df['text'].str.contains(pos_emoticons[0])==False)\n",
    "for i in range(len(pos_emoticons)-1):\n",
    "  dum = (df['sentiment']==1) & (df['text'].str.contains(pos_emoticons[i+1])==False)\n",
    "  pos_drop = pos_drop & dum\n",
    "  \n",
    "neg_drop = (df['sentiment']==-1) & (df['text'].str.contains(neg_emoticons[0])==False)\n",
    "for i in range(len(neg_emoticons)-1):\n",
    "  dum = (df['sentiment']==-1) & (df['text'].str.contains(neg_emoticons[i+1])==False)\n",
    "  neg_drop = neg_drop & dum\n",
    "\n",
    "droplist = list(pos_drop | neg_drop)\n",
    "not_droplist = [not i for i in droplist]\n",
    "df = df[list(not_droplist)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print 'tweets to be dropped: ' + str(sum(droplist)) + ', ' + str(100*float(sum(droplist))/len(df)) + '%'\n",
    "print '  neutral tweets to be dropped: ' + str(sum(neutral_drop)) + ', ' + str(100*float(sum(neutral_drop))/sum(df['sentiment']==0)) + '%'\n",
    "print '  positive tweets to be dropped: ' + str(sum(pos_drop)) + ', ' + str(100*float(sum(pos_drop))/sum(df['sentiment']==1)) + '%'\n",
    "print '  negative tweets to be dropped: ' + str(sum(neg_drop)) + ', ' + str(100*float(sum(neg_drop))/sum(df['sentiment']==-1)) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "## check: none of the wrong emoticons show up\n",
    "print [sum((df['sentiment']==0) & (df['text'].str.contains(e))) for e in emoticons]\n",
    "print [sum((df['sentiment']==1) & (df['text'].str.contains(e))) for e in neg_emoticons]\n",
    "print [sum((df['sentiment']==-1) & (df['text'].str.contains(e))) for e in pos_emoticons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets: 812447\n",
      "  number of neutral tweets: 322689, 39.7181600769%\n",
      "  number of positive tweets: 238593, 29.3672079533%\n",
      "  number of negative tweets: 251165, 30.9146319698%\n"
     ]
    }
   ],
   "source": [
    "## print out some basic final statistics\n",
    "print 'number of tweets: ' + str(len(df))\n",
    "print '  number of neutral tweets: ' + str(sum(df['sentiment']==0)) + ', ' + str(float(sum(df['sentiment']==0))/len(df)*100) + '%'\n",
    "print '  number of positive tweets: ' + str(sum(df['sentiment']==1)) + ', ' + str(float(sum(df['sentiment']==1))/len(df)*100) + '%'\n",
    "print '  number of negative tweets: ' + str(sum(df['sentiment']==-1)) + ', ' + str(float(sum(df['sentiment']==-1))/len(df)*100) + '%'\n",
    "\n",
    "if sum(df['sentiment']==-1) + sum(df['sentiment']==0) + sum(df['sentiment']==1) != len(df):\n",
    "  print 'Warning: not all tweets classified as 1,0,-1!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Break a tweet up into words (unigrams) and convert URL's, @handles to tokens\n",
    "#Why not do this for hashtags also? is it b/c they are less rare, so they can be constituted as words?\n",
    "words = []\n",
    "hashtags = []\n",
    "for i in range(len(df)):\n",
    "  smallwords = df['text'].iloc[i].split()\n",
    "  for j in range(len(smallwords)):\n",
    "    if smallwords[j].startswith('@'):\n",
    "      smallwords[j] = 'USERNAME_TOKEN'\n",
    "    if smallwords[j].startswith('https:'):\n",
    "      smallwords[j] = 'URL_TOKEN'\n",
    "  words.append(smallwords)\n",
    "  hashtags.append( [x for x in df['text'].iloc[i].split() if x.startswith('#')] )\n",
    "df['words'] = words\n",
    "df['hashtags'] = hashtags\n",
    "words_tot = [item for sublist in words for item in sublist]\n",
    "hashtags_tot = [item for sublist in hashtags for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "unicode_pos_emoticons = [u':)', u':-)', u':D', u': )', u'=)']\n",
    "unicode_neg_emoticons = [u':(', u':-(', u': (']\n",
    "unicode_emoticons = unicode_pos_emoticons + unicode_neg_emoticons\n",
    "print unicode_pos_emoticons[0].encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.337769073727\n",
      "[u'@JessFoster15', u'@WomenSportsWeek', u'Hi', u'Jess', u'-', u'22nd', u'Sept', u'-', u'you', u'have', u'plenty', u'of', u'time', u'yet']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pThreshold = 0.9\n",
    "smallwords = df['text'].iloc[400002].split()\n",
    "XXX =random.random()\n",
    "print XXX\n",
    "for s in smallwords:\n",
    "  if s in unicode_emoticons and XXX < pThreshold:\n",
    "    smallwords.remove(s)\n",
    "print smallwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USERNAME_TOKEN', 'USERNAME_TOKEN', u'Hi', u'Jess', u'-', u'22nd', u'Sept', u'-', u'you', u'have', u'plenty', u'of', u'time', u'yet', u':)']\n"
     ]
    }
   ],
   "source": [
    "print df['words'].iloc[400002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "for label, data in (('Word', words_tot), ('Hashtag', hashtags_tot)):\n",
    "  pt = PrettyTable(field_names=[label, 'Count']) \n",
    "  c = Counter(data)\n",
    "  [ pt.add_row(kv) for kv in c.most_common()[:20] ]\n",
    "  pt.align[label], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "  print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## a function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens) \n",
    "print lexical_diversity(words_tot)\n",
    "print lexical_diversity(hashtags_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'average # of hashtags per tweet: ' + str(float(sum(df['hashtags'].apply(len)))/len(df))\n",
    "print 'average # of (non-unique) words per tweet: ' + str(float(sum(df['words'].apply(len)))/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words_tot_counts = sorted(Counter(words_tot).values(), reverse=True)\n",
    "plt.loglog(words_tot_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## histograms (these might only be sensible once I strip some stuff off)\n",
    "\n",
    "for label, data in (('Words', words_tot), ('Hashtags', hashtags_tot)):\n",
    "  # Build a frequency map for each set of data\n",
    "  # and plot the values\n",
    "  c = Counter(data)\n",
    "  plt.hist(c.values())\n",
    "  \n",
    "  # Add a title and y-label ...\n",
    "  plt.title(label)\n",
    "  plt.ylabel(\"Number of items in bin\")\n",
    "  plt.xlabel(\"Bins (number of times an item appeared)\") \n",
    "  \n",
    "  # ... and display as a new figure\n",
    "  plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maybe to do's\n",
    "#normalize' the text, remove cases\n",
    "#count retweets? will be skewed by neutral feeds being major twitter feeds\n",
    "#retweets provide info about users that may be outside our data set (since we can see retweet count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load the sentiment140 tweets\n",
    "s140df = pd.read_csv('tweet_data/sentiment140/training.1600000.processed.noemoticon.csv', names=['sentiment','id','created_at','query','screen_name','text'])\n",
    "## they classified sentiment as 0=negative, 2=neutral, 4=positive, modify their data to reflect my conventions\n",
    "s140df['sentiment'] = (s140df['sentiment'] - 2)/2\n",
    "print 'number of negative tweets: ' + str(sum(s140df['sentiment']==-1))\n",
    "print 'number of neutral tweets: ' + str(sum(s140df['sentiment']==0))\n",
    "print 'number of positive tweets: ' + str(sum(s140df['sentiment']==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####UNUSED OLD CODE\n",
    "## convert URL's to URL_TOKEN\n",
    "#text = list(df['text'])\n",
    "#stripped_text = []\n",
    "#url_counter = 0\n",
    "#for i in range(len(df)):\n",
    "#  dum = text[i]\n",
    "#  while 'https://' in dum:\n",
    "#    dum = dum[:dum.index('https://')] + 'URL_TOKEN' + dum[dum.index('https://')+23:]\n",
    "#    url_counter += 1\n",
    "#  stripped_text.append(dum)\n",
    "#df['text'] = stripped_text\n",
    "#print 'avg. number of urls per tweet: ' + str(float(url_counter)/len(df))\n",
    "\n",
    "## Break a tweet up into words (unigrams) and convert URL's, @handles to tokens\n",
    "words = []\n",
    "hashtags = []\n",
    "for i in range(len(df)):\n",
    "  words.append(df['text'].iloc[i].split())\n",
    "  hashtags.append( [x for x in df['text'].iloc[i].split() if x.startswith('#')] )\n",
    "df['words'] = words\n",
    "df['hashtags'] = hashtags\n",
    "words_tot = [item for sublist in words for item in sublist]\n",
    "hashtags_tot = [item for sublist in hashtags for item in sublist]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2, Gavin",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
