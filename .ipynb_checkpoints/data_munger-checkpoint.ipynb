{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## load the dataframe of (mostly) raw tweets\n",
    "df = pd.read_pickle('tweet_data/tweets.pkl')\n",
    "\n",
    "###TODO: \n",
    "##fix text wrapping\n",
    "## goal of this notebook: mung the data to produce something like col1: sentiment, col2: tweet's words\n",
    "## then apply diff ML methods to this processed data in other notebooks\n",
    "\n",
    "## worry about 'RT's and 'via's'\n",
    "## worry about punctuation\n",
    "## count retweets? will be skewed by neutral feeds being major twitter feeds\n",
    "## retweets provide info about users that may be outside our data set (since we can see retweet count)\n",
    "## might want to count all emoticons as the same? play with this\n",
    "\n",
    "## next, worry about numbers and dates\n",
    "# how to handle contractions?/internet speak like b4\n",
    "\n",
    "##implement baseline\n",
    "#from nltk import NaiveBayesClassifier, classify\n",
    "#why do they call it multinomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of tweets: 952152\n",
      "  number of neutral tweets: 338417, 35.542329376%\n",
      "  number of positive tweets: 306791, 32.2208008805%\n",
      "  number of negative tweets: 306944, 32.2368697435%\n"
     ]
    }
   ],
   "source": [
    "## take fraction of the data and scramble it\n",
    "frac = 1.0\n",
    "df = df.iloc[random.sample(range(0, len(df)), int(math.floor(frac*len(df))))]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## print out some basic initial statistics\n",
    "print 'initial number of tweets: ' + str(len(df))\n",
    "print '  number of neutral tweets: ' + str(sum(df['sentiment']==0)) + ', ' + str(float(sum(df['sentiment']==0))/len(df)*100) + '%'\n",
    "print '  number of positive tweets: ' + str(sum(df['sentiment']==1)) + ', ' + str(float(sum(df['sentiment']==1))/len(df)*100) + '%'\n",
    "print '  number of negative tweets: ' + str(sum(df['sentiment']==-1)) + ', ' + str(float(sum(df['sentiment']==-1))/len(df)*100) + '%'\n",
    "\n",
    "if sum(df['sentiment']==-1) + sum(df['sentiment']==0) + sum(df['sentiment']==1) != len(df):\n",
    "  print 'Not all tweets classified as 1,0,-1!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates deleted: 6306\n",
      "current number of tweets: 945846\n"
     ]
    }
   ],
   "source": [
    "## delete the duplicates\n",
    "Noriginal = len(df)\n",
    "df = df.drop_duplicates(subset='id', keep='last')\n",
    "print 'duplicates deleted: ' + str(Noriginal - len(df))\n",
    "df = df.reset_index(drop=True)\n",
    "print 'current number of tweets: ' + str(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of retweets deleted: 239084\n"
     ]
    }
   ],
   "source": [
    "## delete the retweets\n",
    "print 'number of retweets deleted: ' + str(sum(list(df['retweet'])))\n",
    "df = df[[not i for i in list(df['retweet'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-English tweets deleted: 4111\n",
      "current number of tweets: 702651\n"
     ]
    }
   ],
   "source": [
    "## drop the non-English tweets (most dropped tweets seem to actually be English...)\n",
    "lang_drop = (df['lang']!='en')\n",
    "print 'non-English tweets deleted: ' + str(sum(lang_drop))\n",
    "df = df[list([not i for i in lang_drop])]\n",
    "df = df.reset_index(drop=True)\n",
    "print 'current number of tweets: ' + str(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Preprocessing: set-up\n",
    "\n",
    "## define a preprocessing function that 1) lowers the case, 2) tokenizes into words, 3) lemmatizes\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(sentence):\n",
    "  return [wordnet_lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "## load the list of english stop words\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "## set-up regular expressions to delete all non a-z characters\n",
    "import re\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "## translation tables for dropping punctuation\n",
    "## treat apostrophes differently from things like .,! also do not include _\n",
    "import string\n",
    "punctuation = string.punctuation\n",
    "#punctuation = punctuation.replace('_', '')\n",
    "punctuation_nospace = punctuation[6]\n",
    "punctuation_space = [p for p in punctuation if p not in punctuation_nospace]\n",
    "translate_table_nospace = dict((ord(char), None) for char in punctuation_nospace)   \n",
    "translate_table_space = dict((ord(char), u' ') for char in punctuation_space)\n",
    "\n",
    "## merge the tables\n",
    "translate_table = translate_table_space.copy()\n",
    "translate_table.update(translate_table_nospace) \n",
    "\n",
    "## load the list of emoticons\n",
    "import codecs\n",
    "f = codecs.open('emoticons_positive.txt', encoding='utf-8')\n",
    "pos_emoticons = []\n",
    "for line in f:\n",
    "  pos_emoticons.append(line[0:-1])\n",
    "f = codecs.open('emoticons_negative.txt', encoding='utf-8')\n",
    "neg_emoticons = []\n",
    "for line in f:\n",
    "  neg_emoticons.append(line[0:-1])\n",
    "all_emoticons = pos_emoticons + neg_emoticons\n",
    "ASCII_all_emoticons = [e.encode(\"ascii\",\"ignore\") for e in all_emoticons]\n",
    "\n",
    "## Preprocessing: implementation\n",
    "words = []\n",
    "new_text = []\n",
    "for i in range(len(df)):\n",
    "  text_dummy = df['text'].iloc[i]\n",
    "\n",
    "  ## tokenize positive and negative emoticons\n",
    "  ## also pad the token with spaces to catch even embedded tweets such as 'lame:('\n",
    "  for j in range(len(pos_emoticons)):\n",
    "    text_dummy = text_dummy.replace(pos_emoticons[j], ' posemoticontoken' + ' ')\n",
    "  for j in range(len(neg_emoticons)):\n",
    "    text_dummy = text_dummy.replace(neg_emoticons[j], ' negemoticontoken' + ' ')\n",
    "  \n",
    "  ## tokenize handles, hashtags, and url's\n",
    "  text_dummy = text_dummy.replace('@', ' usernametoken') #this catches the '.@XXX' trick\n",
    "  text_dummy = text_dummy.replace('#', ' hashtagtoken')\n",
    "  while 'https://' in text_dummy:\n",
    "    text_dummy = text_dummy[:text_dummy.index('https://')] +' urltoken ' + text_dummy[text_dummy.index('https://')+23:]\n",
    "  while 'http://' in text_dummy:\n",
    "    text_dummy = text_dummy[:text_dummy.index('http://')] +' urltoken ' + text_dummy[text_dummy.index('http://')+22:]\n",
    "  \n",
    "  ## replace html entities with space (note: 'and' is a stopword and so is dropped, so no need to treat & special)\n",
    "  html_entities = [u'&nbsp;', u'&lt;', u'&gt;', u'&amp;', u'&quot;', u'&apos;', u'&cent;', u'&pound;', u'&yen;', u'&euro;', u'&copy;', u'&reg;']\n",
    "  for html in html_entities:\n",
    "    text_dummy = text_dummy.replace(html, '')\n",
    "  \n",
    "  ## drop punctuation, tokenize into words, lower the case, and lemmatize\n",
    "  text_dummy = text_dummy.translate(translate_table)\n",
    "  words_dummy = preprocess(text_dummy)\n",
    "\n",
    "  ## remove stop words\n",
    "  words_dummy = [w for w in words_dummy if not w in stoplist]\n",
    "  \n",
    "  ## anonymize the username_tokens\n",
    "  for j in range(len(words_dummy)):\n",
    "    if words_dummy[j][0:13] == 'usernametoken':\n",
    "      words_dummy[j] = 'usernametoken'\n",
    "\n",
    "  ## remove numbers\n",
    "  words_dummy = [regex.sub('', w) for w in words_dummy if len(regex.sub('', w)) > 0]\n",
    "  \n",
    "  ## make a new sentence for easy vectorized manipulations\n",
    "  words.append(words_dummy)\n",
    "  new_text.append(\" \".join(words_dummy))\n",
    "df['words'] = words\n",
    "df['new_text'] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets to be dropped: 15947, 2.26954775557%\n",
      "  neutral tweets to be dropped: 1846, 0.671490045869%\n",
      "  positive tweets to be dropped: 11971, 6.17007787977%\n",
      "  negative tweets to be dropped: 2130, 0.911335213051%\n"
     ]
    }
   ],
   "source": [
    "## Refine sentiment assignment\n",
    "\n",
    "## drop the tweets containing the wrong emoticon tokens\n",
    "neutral_drop1 = (df['sentiment']==0) & (df['new_text'].str.contains('posemoticontoken'))\n",
    "neutral_drop2 = (df['sentiment']==0) & (df['new_text'].str.contains('negemoticontoken'))\n",
    "pos_drop1 = (df['sentiment']==1) & (df['new_text'].str.contains('negemoticontoken'))\n",
    "neg_drop1 = (df['sentiment']==-1) & (df['new_text'].str.contains('posemoticontoken'))\n",
    "\n",
    "## drop any positive/negative tweets not containing the appropriate emoticon token\n",
    "## there are much more positive tweets dropped b/c :P was returned in the search query for :)\n",
    "pos_drop2 = (df['sentiment']==1) & (df['new_text'].str.contains('posemoticontoken')==False)\n",
    "neg_drop2 = (df['sentiment']==-1) & (df['new_text'].str.contains('negemoticontoken')==False)\n",
    "\n",
    "droplist = list(neutral_drop1|neutral_drop2|pos_drop1|pos_drop2|neg_drop1|neg_drop2)\n",
    "not_droplist = [not i for i in droplist]\n",
    "print 'tweets to be dropped: ' + str(sum(droplist)) + ', ' + str(100*float(sum(droplist))/len(df)) + '%'\n",
    "print '  neutral tweets to be dropped: ' + str(sum(list(neutral_drop1|neutral_drop2))) + ', ' + str(100*float(sum(list(neutral_drop1|neutral_drop2)))/sum(df['sentiment']==0)) + '%'\n",
    "print '  positive tweets to be dropped: ' + str(sum(list(pos_drop1|pos_drop2))) + ', ' + str(100*float(sum(list(pos_drop1|pos_drop2)))/sum(df['sentiment']==1)) + '%'\n",
    "print '  negative tweets to be dropped: ' + str(sum(list(neg_drop1|neg_drop2))) + ', ' + str(100*float(sum(list(neg_drop1|neg_drop2)))/sum(df['sentiment']==-1)) + '%'\n",
    "df = df[list(not_droplist)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "## drop 'new_text' to avoid mistakes as I continue to process 'words'\n",
    "df.drop('new_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 100.0% of emoticons\n"
     ]
    }
   ],
   "source": [
    "## throw away some fraction (to be treated as a hyperparameter) of the emoticons\n",
    "pThreshold = 0.0 #fraction to keep\n",
    "emoticoncounter = 0\n",
    "droppedcounter = 0\n",
    "for i in range(len(df)):\n",
    "  for w in df['words'].iloc[i]:\n",
    "    if w == 'posemoticontoken' or w == 'negemoticontoken':\n",
    "      emoticoncounter += 1\n",
    "      if random.random() > pThreshold:\n",
    "        df['words'].iloc[i].remove(w)\n",
    "        droppedcounter += 1\n",
    "print 'dropped ' + str(100*float(droppedcounter)/emoticoncounter) + '% of emoticons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets: 686704\n",
      "  number of neutral tweets: 273065, 39.7645856148%\n",
      "  number of positive tweets: 182046, 26.5101120716%\n",
      "  number of negative tweets: 231593, 33.7253023137%\n"
     ]
    }
   ],
   "source": [
    "## print out some basic statistics\n",
    "print 'number of tweets: ' + str(len(df))\n",
    "print '  number of neutral tweets: ' + str(sum(df['sentiment']==0)) + ', ' + str(float(sum(df['sentiment']==0))/len(df)*100) + '%'\n",
    "print '  number of positive tweets: ' + str(sum(df['sentiment']==1)) + ', ' + str(float(sum(df['sentiment']==1))/len(df)*100) + '%'\n",
    "print '  number of negative tweets: ' + str(sum(df['sentiment']==-1)) + ', ' + str(float(sum(df['sentiment']==-1))/len(df)*100) + '%'\n",
    "\n",
    "if sum(df['sentiment']==-1) + sum(df['sentiment']==0) + sum(df['sentiment']==1) != len(df):\n",
    "  print 'Warning: not all tweets classified as 1,0,-1!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "| Word          |  Count |\n",
      "+---------------+--------+\n",
      "| urltoken      | 434682 |\n",
      "| usernametoken | 414200 |\n",
      "| im            |  39232 |\n",
      "| u             |  37233 |\n",
      "| thanks        |  29832 |\n",
      "| like          |  23590 |\n",
      "| wa            |  23586 |\n",
      "| get           |  23574 |\n",
      "| new           |  21811 |\n",
      "| want          |  21674 |\n",
      "| day           |  21329 |\n",
      "| dont          |  20642 |\n",
      "| one           |  19278 |\n",
      "| love          |  19152 |\n",
      "| good          |  16919 |\n",
      "| miss          |  16796 |\n",
      "| time          |  16311 |\n",
      "| see           |  16216 |\n",
      "| great         |  16006 |\n",
      "| much          |  15912 |\n",
      "| know          |  15856 |\n",
      "| happy         |  15407 |\n",
      "| go            |  15271 |\n",
      "| make          |  14119 |\n",
      "| today         |  13743 |\n",
      "| need          |  13579 |\n",
      "| ha            |  13258 |\n",
      "| cant          |  13239 |\n",
      "| thank         |  13004 |\n",
      "| say           |  12770 |\n",
      "| week          |  12412 |\n",
      "| follow        |  12342 |\n",
      "| year          |  12136 |\n",
      "| help          |  11787 |\n",
      "| back          |  11525 |\n",
      "| sorry         |  11467 |\n",
      "| na            |  11462 |\n",
      "| really        |  11356 |\n",
      "| please        |  11023 |\n",
      "| youre         |  10848 |\n",
      "| hope          |  10680 |\n",
      "| got           |  10557 |\n",
      "| people        |  10366 |\n",
      "| work          |  10212 |\n",
      "| look          |  10032 |\n",
      "| still         |   9969 |\n",
      "| trump         |   9642 |\n",
      "| well          |   9544 |\n",
      "| via           |   9135 |\n",
      "| sad           |   8947 |\n",
      "+---------------+--------+\n"
     ]
    }
   ],
   "source": [
    "## examine word frequencies\n",
    "words_tot = [item for sublist in df['words'] for item in sublist]\n",
    "from prettytable import PrettyTable\n",
    "pt = PrettyTable(field_names=['Word', 'Count']) \n",
    "c = Counter(words_tot)\n",
    "[ pt.add_row(kv) for kv in c.most_common()[:50] ]\n",
    "pt.align['Word'], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0305169647314\n"
     ]
    }
   ],
   "source": [
    "## lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens) \n",
    "print lexical_diversity(words_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fce9a3d6650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhkShoiIhKZkoaIiET2/7fMIlLZ05idAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce903c0ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words_tot_counts = sorted(Counter(words_tot).values(), reverse=True)\n",
    "plt.loglog(words_tot_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## save the processed data\n",
    "df_final = df.copy(deep=True)\n",
    "df_final = df_final.drop([l for l in list(df.columns.values) if l not in ['sentiment','words']], axis=1)\n",
    "df_final.to_pickle('tweet_data/tweets_munged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### load the sentiment140 tweets\n",
    "#s140df = pd.read_csv('tweet_data/sentiment140/training.1600000.processed.noemoticon.csv', names=['sentiment','id','created_at','query','screen_name','text'])\n",
    "### they classified sentiment as 0=negative, 2=neutral, 4=positive, modify their data to reflect my conventions\n",
    "#s140df['sentiment'] = (s140df['sentiment'] - 2)/2\n",
    "#print 'number of negative tweets: ' + str(sum(s140df['sentiment']==-1))\n",
    "#print 'number of neutral tweets: ' + str(sum(s140df['sentiment']==0))\n",
    "#print 'number of positive tweets: ' + str(sum(s140df['sentiment']==1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2, Gavin",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
