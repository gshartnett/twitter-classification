{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## To Do\n",
    "# worry about how I treat \"new words\"\n",
    "# think about evaluation beyond classification errror\n",
    "# re-evaluate how I do the preprocessing\n",
    "## also plot CV LL\n",
    "## consider bigrams\n",
    "## Look at error rate for +,0,- separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data set size: 15183\n",
      "training-cv data set size: 1687\n",
      "Sanders anlaytics (SA) cv data set size: 2624\n",
      "Sentiment 140 (S140) cv data set size: 497\n",
      "test data set size: 95322\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import scipy.sparse\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "\n",
    "## load the processed training-set tweets\n",
    "df = pd.read_pickle('tweet_data/training_data/training_data_processed.pkl')\n",
    "df_cv = pd.read_pickle('tweet_data/training_data/training_data_cv_processed.pkl')\n",
    "\n",
    "## load the processed cv-set tweets\n",
    "df_sa_cv = pd.read_pickle('tweet_data/cv_data/sanders_analytics/sanders_analytics_cv_data_processed.pkl')\n",
    "df_s140_cv = pd.read_pickle('tweet_data/cv_data/sentiment140/sentiment140_cv_data_processed.pkl')\n",
    "\n",
    "## load the processed test-set tweets\n",
    "df_test = pd.read_pickle('tweet_data/test_data/debate_data/debate_data_processed.pkl')\n",
    "\n",
    "print 'training data set size:', len(df)\n",
    "print 'training-cv data set size:', + len(df_cv)\n",
    "print 'Sanders anlaytics (SA) cv data set size:', len(df_sa_cv)\n",
    "print 'Sentiment 140 (S140) cv data set size:', len(df_s140_cv)\n",
    "print 'test data set size:', len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training emoticon fractions: [0.06159527326440177, 0.07155025553662692]\n",
      "training-CV emoticon fractions: [0.06495263870094722, 0.05902192242833052]\n",
      "SA CV emoticon fractions: [0.024282560706401765, 0.05089058524173028]\n",
      "S140 CV emoticon fractions: [0.06779661016949153, 0.11049723756906077]\n"
     ]
    }
   ],
   "source": [
    "## some useful functions\n",
    "\n",
    "## sentiment statistics\n",
    "def sentiment_stats(DF):\n",
    "  return [1.0*sum(DF['sentiment']==-1)/len(DF), 1.0*sum(DF['sentiment']==0)/len(DF), 1.0*sum(DF['sentiment']==1)/len(DF)]\n",
    "\n",
    "## emoticon statistics\n",
    "def emoticon_stats(DF):\n",
    "  neg_emoticon_counter = 0\n",
    "  pos_emoticon_counter = 0\n",
    "  for i in range(len(DF)):\n",
    "    if DF['sentiment'].iloc[i] == -1:\n",
    "      neg_emoticon_counter += Counter(DF['words'].iloc[i])['negemoticontoken']\n",
    "    if DF['sentiment'].iloc[i] == 1:\n",
    "      pos_emoticon_counter += Counter(DF['words'].iloc[i])['posemoticontoken']\n",
    "  return [float(neg_emoticon_counter)/sum(DF['sentiment']==-1), float(pos_emoticon_counter)/sum(DF['sentiment']==1)]\n",
    "\n",
    "print 'training emoticon fractions:', emoticon_stats(df)\n",
    "print 'training-CV emoticon fractions:', emoticon_stats(df_cv)\n",
    "print 'SA CV emoticon fractions:', emoticon_stats(df_sa_cv)\n",
    "print 'S140 CV emoticon fractions:', emoticon_stats(df_s140_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with df_cv\n",
      "done with df_sa_cv\n",
      "done with df_s140_cv\n",
      "done with df_test\n"
     ]
    }
   ],
   "source": [
    "## build the bag of words\n",
    "ranks_tot = [item for sublist in df['word_ranks'] for item in sublist]\n",
    "ranks_tot_neg = [item for sublist in df['word_ranks'][df['sentiment'] == -1] for item in sublist]\n",
    "ranks_tot_neutral = [item for sublist in df['word_ranks'][df['sentiment'] == 0] for item in sublist]\n",
    "ranks_tot_pos = [item for sublist in df['word_ranks'][df['sentiment'] == 1] for item in sublist]\n",
    "counter = Counter(ranks_tot)\n",
    "counter_neg = Counter(ranks_tot_neg)\n",
    "counter_neutral = Counter(ranks_tot_neutral)\n",
    "counter_pos = Counter(ranks_tot_pos)\n",
    "max_rank = max(ranks_tot)+1\n",
    "\n",
    "## put the CV sets into word rank form\n",
    "words_tot = [item for sublist in df['words'] for item in sublist]\n",
    "count = Counter(words_tot).most_common()\n",
    "words_unique = [count[i][0] for i in range(len(count))]\n",
    "words_frequencies = [count[i][1] for i in range(len(count))]\n",
    "\n",
    "def word_to_rank(x):\n",
    "  return sorted([ words_unique.index(w) for w in x if w in words_unique])\n",
    "\n",
    "df_cv['word_ranks'] = df_cv['words'].map(word_to_rank)\n",
    "print 'done with df_cv'\n",
    "df_sa_cv['word_ranks'] = df_sa_cv['words'].map(word_to_rank)\n",
    "print 'done with df_sa_cv'\n",
    "df_s140_cv['word_ranks'] = df_s140_cv['words'].map(word_to_rank)\n",
    "print 'done with df_s140_cv'\n",
    "df_test['word_ranks'] = df_test['words'].map(word_to_rank)\n",
    "print 'done with df_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV set: correctly classified: 75.1037344398 %\n",
      "SA V set: correctly classified: 41.6539634146 %\n",
      "S140 CV set: correctly classified: 58.953722334 %\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "## Naive Bayes classifier\n",
    "\n",
    "##re-think Laplace smoothing now that I'm ignoring new words in cv set\n",
    "\n",
    "## priors\n",
    "prior_neg = float(sum(df['sentiment']==-1))/len(df)\n",
    "prior_neutral = float(sum(df['sentiment']==0))/len(df)\n",
    "prior_pos = float(sum(df['sentiment']==1))/len(df)\n",
    "\n",
    "## conditional probabilities of word given class (with Laplace smoothing parameter alpha)\n",
    "alpha = 1 # Laplace smoothing parameter\n",
    "def p_rank_given_neg(rank):\n",
    "  return float(counter_neg[rank] + alpha)/(len(ranks_tot_neg) + alpha*len(ranks_tot))\n",
    "def p_rank_given_neutral(rank):\n",
    "  return float(counter_neutral[rank] + alpha)/(len(ranks_tot_neutral) + alpha*len(ranks_tot))\n",
    "def p_rank_given_pos(rank):\n",
    "  return float(counter_pos[rank] + alpha)/(len(ranks_tot_pos) + alpha*len(ranks_tot))\n",
    "\n",
    "## class prediction\n",
    "def NB_predict(rank_list):\n",
    "  p_neg = math.log(prior_neg) + sum([math.log(p_rank_given_neg(i)) for i in rank_list])\n",
    "  p_neutral = math.log(prior_neutral) + sum([math.log(p_rank_given_neutral(i)) for i in rank_list])\n",
    "  p_pos = math.log(prior_pos) + sum([math.log(p_rank_given_pos(i)) for i in rank_list])\n",
    "  return -1 + [p_neg, p_neutral, p_pos].index(max([p_neg, p_neutral, p_pos]))\n",
    "\n",
    "## classification error\n",
    "print 'CV set: correctly classified:', 100*float(sum(df_cv['word_ranks'].map(NB_predict) == df_cv['sentiment']))/len(df_cv), '%'\n",
    "print 'SA V set: correctly classified:', 100*float(sum(df_sa_cv['word_ranks'].map(NB_predict) == df_sa_cv['sentiment']))/len(df_sa_cv), '%'\n",
    "print 'S140 CV set: correctly classified:', 100*float(sum(df_s140_cv['word_ranks'].map(NB_predict) == df_s140_cv['sentiment']))/len(df_s140_cv), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "## MaxEnt Classifier\n",
    "\n",
    "## construct sentiment indicator vectors and the sparse feature matrix for all data sets\n",
    "I_neg = np.array(df['sentiment']==-1, dtype=float)\n",
    "I_neutral = np.array(df['sentiment']==0, dtype=float)\n",
    "I_pos = np.array(df['sentiment']==1, dtype=float)\n",
    "\n",
    "## create the sparse feature matrices\n",
    "def feature_matrix(df):\n",
    "  feature = scipy.sparse.lil_matrix((len(df), max_rank))\n",
    "  for j in range(len(df)):\n",
    "    for i in range(len(df['word_ranks'].iloc[j])):\n",
    "      feature[j, df['word_ranks'].iloc[j][i]] += 1.0/(len(df['word_ranks'].iloc[j]))\n",
    "  return scipy.sparse.csr_matrix(feature) ## convert into csr matrix\n",
    "\n",
    "feature = feature_matrix(df)\n",
    "feature_cv = feature_matrix(df_cv)\n",
    "feature_sa_cv = feature_matrix(df_sa_cv)\n",
    "feature_s140_cv = feature_matrix(df_s140_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## useful functions\n",
    "\n",
    "## compute the log likelihood\n",
    "def loglike(lambda_neg, lambda_neutral, lambda_pos, flambda_neg, flambda_neutral, flambda_pos):\n",
    "  return flambda_neg.dot(I_neg) + flambda_neutral.dot(I_neutral) + flambda_pos.dot(I_pos) \\\n",
    "    - sum(np.log(np.exp(flambda_neg) + np.exp(flambda_neutral) + np.exp(flambda_pos)))\n",
    "\n",
    "## compute the gradient\n",
    "def gradient(lambda_neg, lambda_neutral, lambda_pos, flambda_neg, flambda_neutral, flambda_pos):\n",
    "  boltz_neg = np.exp(flambda_neg)\n",
    "  boltz_neutral = np.exp(flambda_neutral)\n",
    "  boltz_pos = np.exp(flambda_pos)\n",
    "  dLLdlambda_neg = feature.transpose().dot(I_neg) - feature.transpose().dot( (boltz_neg/(boltz_neg + boltz_neutral + boltz_pos) ) )\n",
    "  dLLdlambda_neutral = feature.transpose().dot(I_neutral) - feature.transpose().dot( (boltz_neutral/(boltz_neg + boltz_neutral + boltz_pos) ) )\n",
    "  dLLdlambda_pos = feature.transpose().dot(I_pos) - feature.transpose().dot( (boltz_pos/(boltz_neg + boltz_neutral + boltz_pos) ) )\n",
    "  return [dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos]\n",
    "\n",
    "## prediction accuracy\n",
    "def ME_predictions(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos):\n",
    "  flambda_neg_cv = feature_cv.dot(lambda_neg)\n",
    "  flambda_neutral_cv = feature_cv.dot(lambda_neutral)\n",
    "  flambda_pos_cv = feature_cv.dot(lambda_pos)\n",
    "  boltz_neg_cv = np.squeeze(np.asarray(np.exp(flambda_neg_cv)))\n",
    "  boltz_neutral_cv = np.squeeze(np.asarray(np.exp(flambda_neutral_cv)))\n",
    "  boltz_pos_cv = np.squeeze(np.asarray(np.exp(flambda_pos_cv)))\n",
    "  predict = []\n",
    "  for i in range(len(df_cv)):\n",
    "    p_neg = boltz_neg_cv[i]\n",
    "    p_neutral = boltz_neutral_cv[i]\n",
    "    p_pos = boltz_pos_cv[i]\n",
    "    predict.append(-1 + [p_neg, p_neutral, p_pos].index(max([p_neg, p_neutral, p_pos])))\n",
    "  return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "## learning rate\n",
    "alpha = 0.0001\n",
    "\n",
    "## create the vector of parameters to be learned (initialized to zero)\n",
    "lambda_neg = np.zeros(max_rank)\n",
    "lambda_neutral = np.zeros(max_rank)\n",
    "lambda_pos = np.zeros(max_rank)\n",
    "## the initial cost and accuracy\n",
    "flambda_neg = feature.dot(lambda_neg)\n",
    "flambda_neutral = feature.dot(lambda_neutral)\n",
    "flambda_pos = feature.dot(lambda_pos)\n",
    "## record the log likelihood and the prediction\n",
    "LL_history = [loglike(lambda_neg, lambda_neutral, lambda_pos, flambda_neg, flambda_neutral, flambda_pos)]\n",
    "predictions_cv_history = [100.0*sum(ME_predictions(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos) == df_cv['sentiment'])/len(df_cv)]\n",
    "predictions_sa_cv_history = [100.0*sum(ME_predictions(df_sa_cv, feature_sa_cv, lambda_neg, lambda_neutral, lambda_pos) == df_sa_cv['sentiment'])/len(df_sa_cv)]\n",
    "predictions_s140_cv_history = [100.0*sum(ME_predictions(df_s140_cv, feature_s140_cv, lambda_neg, lambda_neutral, lambda_pos) == df_s140_cv['sentiment'])/len(df_s140_cv)]\n",
    "\n",
    "## gradient ascent\n",
    "Nitermax = 300\n",
    "for i in range(Nitermax):\n",
    "  if math.floor(10.0*(i+1)/Nitermax) > math.floor(10.0*i/Nitermax):\n",
    "    #print 'training:', str(i+1), 'out of', Nitermax\n",
    "    print 'training:', str(int(math.floor(100.0*(i+1)/Nitermax))) +'% complete'\n",
    "  ## compute the gradient\n",
    "  [dLLdlambda_neg, dLLdlambda_neutral, dLLdlambda_pos] = gradient(lambda_neg, lambda_neutral, lambda_pos, flambda_neg, flambda_neutral, flambda_pos)\n",
    "  ## advance the parameters\n",
    "  lambda_neg = lambda_neg + alpha*dLLdlambda_neg\n",
    "  lambda_neutral = lambda_neutral + alpha*dLLdlambda_neutral\n",
    "  lambda_pos = lambda_pos + alpha*dLLdlambda_pos\n",
    "  ## some useful factors\n",
    "  flambda_neg = feature.dot(lambda_neg)\n",
    "  flambda_neutral = feature.dot(lambda_neutral)\n",
    "  flambda_pos = feature.dot(lambda_pos)\n",
    "  ## record the log likelihood and the predictions\n",
    "  LL_history.append(loglike(lambda_neg, lambda_neutral, lambda_pos, flambda_neg, flambda_neutral, flambda_pos))\n",
    "  if LL_history[-1] < LL_history[-2]:\n",
    "    print 'Warning: log likelihood not decreasing!'\n",
    "  predictions_cv_history.append(100.0*sum(ME_predictions(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos) == df_cv['sentiment'])/len(df_cv))\n",
    "  predictions_sa_cv_history.append(100.0*sum(ME_predictions(df_sa_cv, feature_sa_cv, lambda_neg, lambda_neutral, lambda_pos) == df_sa_cv['sentiment'])/len(df_sa_cv))\n",
    "  predictions_s140_cv_history.append(100.0*sum(ME_predictions(df_s140_cv, feature_s140_cv, lambda_neg, lambda_neutral, lambda_pos) == df_s140_cv['sentiment'])/len(df_s140_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cross-Validation Analysis\n",
    "\n",
    "## In the Sentiment 140 Go et al paper, they find accuracies of about 80% for various algorithms.\n",
    "## In arriving at this number, they do not consider neutral tweets. \n",
    "## What do I get if I now, at this final stage, only look at the +/- classification problem?\n",
    "def ME_predictions_cheating(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos):\n",
    "  flambda_neg_cv = feature_cv.dot(lambda_neg)\n",
    "  flambda_neutral_cv = feature_cv.dot(lambda_neutral)\n",
    "  flambda_pos_cv = feature_cv.dot(lambda_pos)\n",
    "  boltz_neg_cv = np.squeeze(np.asarray(np.exp(flambda_neg_cv)))\n",
    "  boltz_neutral_cv = np.squeeze(np.asarray(np.exp(flambda_neutral_cv)))\n",
    "  boltz_pos_cv = np.squeeze(np.asarray(np.exp(flambda_pos_cv)))\n",
    "  predict = []\n",
    "  j1 = 0\n",
    "  j2 = 0\n",
    "  for i in range(len(df_cv)):\n",
    "    predict = -1 + 2*[boltz_neg_cv[i], boltz_pos_cv[i]].index(max([boltz_neg_cv[i], boltz_pos_cv[i]]))\n",
    "    if df_cv['sentiment'].iloc[i] != 0:\n",
    "      j1 += 1\n",
    "      if predict == df_cv['sentiment'].iloc[i]:\n",
    "        j2 += 1\n",
    "  #return the accuracy 1) neglecting neutral tweets entirely 2) considering all neutrals to be mischaracterized\n",
    "  return [100.0*j2/j1, 100.0*j2/len(df_cv)] \n",
    "\n",
    "## examine performance\n",
    "pt = PrettyTable(field_names=['CV set', 'NB accuracy', 'MaxEnt accuracy', 'MaxEnt +/- cheating', 'MaxEnt +/- cheating->honest']) \n",
    "[ pt.add_row(['training CV', 100*float(sum(df_cv['word_ranks'].map(NB_predict) == df_cv['sentiment']))/len(df_cv), predictions_cv_history[-1], \\\n",
    "              ME_predictions_cheating(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos)[0], \\\n",
    "              ME_predictions_cheating(df_cv, feature_cv, lambda_neg, lambda_neutral, lambda_pos)[1] ])]\n",
    "[ pt.add_row(['SA CV', 100*float(sum(df_sa_cv['word_ranks'].map(NB_predict) == df_sa_cv['sentiment']))/len(df_sa_cv), predictions_sa_cv_history[-1], \\\n",
    "              ME_predictions_cheating(df_sa_cv, feature_sa_cv, lambda_neg, lambda_neutral, lambda_pos)[0], \\\n",
    "              ME_predictions_cheating(df_sa_cv, feature_sa_cv, lambda_neg, lambda_neutral, lambda_pos)[1] ])]\n",
    "[ pt.add_row(['S140 CV', 100*float(sum(df_s140_cv['word_ranks'].map(NB_predict) == df_s140_cv['sentiment']))/len(df_s140_cv), predictions_s140_cv_history[-1], \\\n",
    "              ME_predictions_cheating(df_s140_cv, feature_s140_cv, lambda_neg, lambda_neutral, lambda_pos)[0], \\\n",
    "              ME_predictions_cheating(df_s140_cv, feature_s140_cv, lambda_neg, lambda_neutral, lambda_pos)[1] ])]\n",
    "#pt.align['Word'], pt.align['Count'] = 'l', 'r' #set column alignment\n",
    "print pt\n",
    "\n",
    "## plot the training history\n",
    "%matplotlib inline\n",
    "plt.plot(LL_history, c='black')\n",
    "plt.ylabel('log likelihood')\n",
    "plt.xlabel('gradient ascent iterations')\n",
    "plt.savefig('figures/MaxEnt_gradient_ascent_history.png')\n",
    "plt.show()\n",
    "\n",
    "## plot the prediction history\n",
    "%matplotlib inline\n",
    "plt.plot(predictions_cv_history, c='black')\n",
    "plt.plot(predictions_sa_cv_history, c='black')\n",
    "plt.plot(predictions_s140_cv_history, c='black')\n",
    "plt.ylabel('prediction % accuracy')\n",
    "plt.xlabel('gradient ascent iterations')\n",
    "plt.savefig('figures/MaxEnt_CVaccuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Debate Analysis\n",
    "feature_test = feature_matrix(df_test)\n",
    "testpred = ME_predictions(df_test, feature_test, lambda_neg, lambda_neutral, lambda_pos)\n",
    "df_test['predictions'] = testpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Sept. 26 Debate'\n",
    "print 'sentiment:', 1.0*sum(df_test[df_test['event date']=='sept26']['predictions'])/len(df_test[df_test['event date']=='sept26'])\n",
    "print ' positive:', 1.0*sum(df_test[df_test['event date']=='sept26']['predictions']==1)/len(df_test[df_test['event date']=='sept26'])\n",
    "print ' neutral:', 1.0*sum(df_test[df_test['event date']=='sept26']['predictions']==0)/len(df_test[df_test['event date']=='sept26'])\n",
    "print ' negative:', 1.0*sum(df_test[df_test['event date']=='sept26']['predictions']==-1)/len(df_test[df_test['event date']=='sept26'])\n",
    "\n",
    "print 'Oct. 9 Debate'\n",
    "print 'sentiment:', 1.0*sum(df_test[df_test['event date']=='oct09']['predictions'])/len(df_test[df_test['event date']=='oct09'])\n",
    "print ' positive:', 1.0*sum(df_test[df_test['event date']=='oct09']['predictions']==1)/len(df_test[df_test['event date']=='oct09'])\n",
    "print ' neutral:', 1.0*sum(df_test[df_test['event date']=='oct09']['predictions']==0)/len(df_test[df_test['event date']=='oct09'])\n",
    "print ' negative:', 1.0*sum(df_test[df_test['event date']=='oct09']['predictions']==-1)/len(df_test[df_test['event date']=='oct09'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot the moving average for the sentiment of each debate\n",
    "\n",
    "## convert the id to a number from 0 to 1\n",
    "def xid(df, eventdate):\n",
    "  return np.asarray(1.0*(df[df['event date'] == eventdate]['id'] - df[df['event date'] == eventdate]['id'].iloc[0]) \\\n",
    "  /(df[df['event date'] == eventdate]['id'].iloc[-1] - df[df['event date'] == eventdate]['id'].iloc[0]))\n",
    "xid_sept26 = xid(df_test, 'sept26')\n",
    "xid_oct09 = xid(df_test, 'oct09')\n",
    "\n",
    "## the sentiment values  \n",
    "def yvalue(df, eventdate, win):\n",
    "  return np.asarray(df[df['event date'] == eventdate]['predictions'].rolling(window=win,center=False).mean())\n",
    "win = 1000\n",
    "y_sept26 = yvalue(df_test, 'sept26', win)\n",
    "y_oct09 = yvalue(df_test, 'oct09', win)\n",
    "\n",
    "## find out when the debates began and ended (1am UTC time)\n",
    "id_sept26start = df_test['id'].iloc[df_test[df_test['created_at'].str.contains(\"Tue Sep 27 01:01\")].index[0]]\n",
    "id_sept26end = df_test['id'].iloc[df_test[df_test['created_at'].str.contains(\"Tue Sep 27 02:31\")].index[0]]\n",
    "id_oct09start = df_test['id'].iloc[df_test[df_test['created_at'].str.contains(\"Mon Oct 10 01:01\")].index[0]]\n",
    "id_oct09end = df_test['id'].iloc[df_test[df_test['created_at'].str.contains(\"Mon Oct 10 02:31\")].index[0]]\n",
    "\n",
    "## find the xid value of a specific id\n",
    "def xid_specific(df, eventdate, specific_id):\n",
    "  return 1.0*(specific_id - df[df['event date'] == eventdate]['id'].iloc[0]) \\\n",
    "  /(df[df['event date'] == eventdate]['id'].iloc[-1] - df[df['event date'] == eventdate]['id'].iloc[0])\n",
    "x_sept26start = xid_specific(df_test, 'sept26', id_sept26start)\n",
    "x_sept26end = xid_specific(df_test, 'sept26', id_sept26end)\n",
    "x_oct09start = xid_specific(df_test, 'oct09', id_oct09start)\n",
    "x_oct09end = xid_specific(df_test, 'oct09', id_oct09end)\n",
    "\n",
    "## make the plots\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "## subplot 1\n",
    "sub1 = fig.add_subplot(121)\n",
    "sub1.set_title('Sept. 26 Debate Avg. Sentiment')\n",
    "sub1.plot(xid_sept26, y_sept26, c='black')\n",
    "sub1.set_xlabel('x')\n",
    "sub1.axvline(x_sept26start, color='k', linestyle='--')\n",
    "sub1.axvline(x_sept26end, color='k', linestyle='--')\n",
    "sub1.plot(np.zeros(2), 'r--')\n",
    "## subplot 2\n",
    "sub2 = fig.add_subplot(122)\n",
    "sub2.set_title('Oct. 09 Debate Avg. Sentiment')\n",
    "sub2.plot(xid_oct09, y_oct09, c='black')\n",
    "sub2.set_xlabel('x')\n",
    "sub2.axvline(x_oct09start, color='k', linestyle='--')\n",
    "sub2.axvline(x_oct09end, color='k', linestyle='--')\n",
    "sub2.plot(np.zeros(2), 'r--')\n",
    "## finish up\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/DebateAvgSent.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## examine tweets that mention just Clinton or just Trump\n",
    "HRCwords = [u'hrc', u'hillary', u'clinton', u'hashtagtokenhillaryclinton', u'hashtagtokenhillary']\n",
    "DJTwords = [u'donald', u'trump', u'hashtagtokendonaldtrump', u'hashtagtokentrumppence']\n",
    "\n",
    "def in_list(list_to_search, terms_to_search, terms_to_exclude):\n",
    "    pos_results = [item for item in list_to_search if item in terms_to_search]\n",
    "    neg_results = [item for item in list_to_search if item in terms_to_exclude]\n",
    "    if len(pos_results) > 0 and len(neg_results) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "print 'Trump only sentiment, Sept. 26:', 1.0*sum(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, HRCwords))) & (df_test['event date'] == 'sept26')]['predictions']) \\\n",
    "  / len( df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, HRCwords))) & (df_test['event date'] == 'sept26')] )\n",
    "  \n",
    "print 'Clinton only sentiment, Sept. 26:', 1.0*sum(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')]['predictions']) \\\n",
    "  / len( df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')] )\n",
    "  \n",
    "print 'Trump only sentiment, Oct. 9:', 1.0*sum(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, HRCwords))) & (df_test['event date'] == 'oct09')]['predictions']) \\\n",
    "  / len( df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, HRCwords))) & (df_test['event date'] == 'oct09')] )\n",
    "  \n",
    "print 'Clinton only sentiment, Oct. 9:', 1.0*sum(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')]['predictions']) \\\n",
    "  / len( df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot the moving average for the sentiment of each debate, \n",
    "## restricting to tweets mentioning only Trump or only Hillary\n",
    "\n",
    "## convert the id to a number from 0 to 1\n",
    "xid_sept26_trump = xid(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')], 'sept26')\n",
    "xid_oct09_trump = xid(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')], 'oct09')\n",
    "xid_sept26_hillary = xid(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'sept26')], 'sept26')\n",
    "xid_oct09_hillary = xid(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'oct09')], 'oct09')\n",
    "\n",
    "## the sentiment values\n",
    "win = 400\n",
    "y_sept26_trump = yvalue(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', win)\n",
    "y_oct09_trump = yvalue(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', win)\n",
    "y_sept26_hillary = yvalue(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', win)\n",
    "y_oct09_hillary = yvalue(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', win)\n",
    "\n",
    "## find the xid value of a specific id\n",
    "x_sept26start_trump = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', id_sept26start)\n",
    "x_sept26end_trump = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', id_sept26end)\n",
    "x_oct09start_trump = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', id_oct09start)\n",
    "x_oct09end_trump = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, DJTwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', id_oct09end)\n",
    "x_sept26start_hillary = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', id_sept26start)\n",
    "x_sept26end_hillary = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'sept26')], 'sept26', id_sept26end)\n",
    "x_oct09start_hillary = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', id_oct09start)\n",
    "x_oct09end_hillary = xid_specific(df_test[(df_test['words'].apply(lambda x: in_list(x, HRCwords, []))) & (df_test['event date'] == 'oct09')], 'oct09', id_oct09end)\n",
    "\n",
    "\n",
    "## make the plots\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "## subplot 1\n",
    "sub1 = fig.add_subplot(221)\n",
    "sub1.set_title('Sept. 26 Debate Avg. Sentiment (Trump)')\n",
    "sub1.plot(xid_sept26_trump, y_sept26_trump, c='black')\n",
    "sub1.set_xlabel('x')\n",
    "sub1.axvline(x_sept26start_trump, color='k', linestyle='--')\n",
    "sub1.axvline(x_sept26end_trump, color='k', linestyle='--')\n",
    "sub1.plot(np.zeros(2), 'r--')\n",
    "## subplot 2\n",
    "sub2 = fig.add_subplot(222)\n",
    "sub2.set_title('Sept. 26 Debate Avg. Sentiment (Hillary)')\n",
    "sub2.plot(xid_sept26_hillary, y_sept26_hillary, c='black')\n",
    "sub2.set_xlabel('x')\n",
    "sub2.axvline(x_sept26start_hillary, color='k', linestyle='--')\n",
    "sub2.axvline(x_sept26start_hillary, color='k', linestyle='--')\n",
    "sub2.plot(np.zeros(2), 'r--')\n",
    "## subplot 3\n",
    "sub1 = fig.add_subplot(223)\n",
    "sub1.set_title('Oct. 09 Debate Avg. Sentiment (Trump)')\n",
    "sub1.plot(xid_oct09_trump, y_oct09_trump, c='black')\n",
    "sub1.set_xlabel('x')\n",
    "sub1.axvline(x_oct09start_trump, color='k', linestyle='--')\n",
    "sub1.axvline(x_oct09end_trump, color='k', linestyle='--')\n",
    "sub1.plot(np.zeros(2), 'r--')\n",
    "## subplot 4\n",
    "sub2 = fig.add_subplot(224)\n",
    "sub2.set_title('Oct. 09 Debate Avg. Sentiment (Hillary)')\n",
    "sub2.plot(xid_oct09_hillary, y_oct09_hillary, c='black')\n",
    "sub2.set_xlabel('x')\n",
    "sub2.axvline(x_oct09start_hillary, color='k', linestyle='--')\n",
    "sub2.axvline(x_oct09end_hillary, color='k', linestyle='--')\n",
    "sub2.plot(np.zeros(2), 'r--')\n",
    "## finish up\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/DebateAvgSent_candidatespecific.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xxxxxxxxxxxxxx OLD CODE ABOUT PARAM SPACE xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sentiment prediction\n",
    "def predict(word_list):\n",
    "  p_neg = math.log(prior_neg) + sum([math.log(p_word_given_neg(i)) for i in word_list])\n",
    "  p_neutral = math.log(prior_neutral) + sum([math.log(p_word_given_neutral(i)) for i in word_list])\n",
    "  p_pos = math.log(prior_pos) + sum([math.log(p_word_given_pos(i)) for i in word_list])\n",
    "  return -1 + [p_neg, p_neutral, p_pos].index(max([p_pos, p_neutral, p_neg]))\n",
    "\n",
    "## sentiment density\n",
    "def s_predict(df):\n",
    "  return sum(1.0*df['predictions'])/len(df)\n",
    "def s_true(df):\n",
    "  return sum(1.0*df['sentiment'])/len(df)\n",
    "\n",
    "## keep a fraction of data\n",
    "def decimate(DF, Aminus, A0, Aplus):\n",
    "  keep_minus = random.sample([k for k, x in enumerate(DF['sentiment']==-1) if x], int(Aminus*sum(DF['sentiment']==-1)))\n",
    "  keep_0 = random.sample([k for k, x in enumerate(DF['sentiment']==0) if x], int(A0*sum(DF['sentiment']==0)))\n",
    "  keep_plus = random.sample([k for k, x in enumerate(DF['sentiment']==1) if x], int(Aplus*sum(DF['sentiment']==1)))\n",
    "  keep = sorted(keep_minus + keep_0 + keep_plus)\n",
    "  return DF.iloc[keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_stats(df_sa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## evaluate error of an ensemble of CV sets in parameter space\n",
    "random.seed(13)\n",
    "m=50\n",
    "grid = []\n",
    "[nminus, n0, nplus] = sentiment_stats(df_s140_cv)\n",
    "a = min(nplus,nminus,n0)\n",
    "for i in range(m):\n",
    "  for j in range(m):\n",
    "    if (i+j <= m-1):\n",
    "      aminus = a*j/(nminus*(m-1))\n",
    "      a0 = a*(1 - float(i+j)/(m-1) )/(1-nplus-nminus)\n",
    "      aplus = a*i/(nplus*(m-1))\n",
    "      df_temp = decimate(df_s140_cv, aminus, a0, aplus)\n",
    "      [nminus2, n02, nplus2] = sentiment_stats(df_temp)\n",
    "      df_temp['predictions'] = df_temp['words'].map(predict)\n",
    "      grid.append( [nplus2, nminus2, s_predict(df_temp), s_true(df_temp)] )\n",
    "    #else:\n",
    "    #  grid.append( [nplus2, nminus2, 0, 0] )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot the grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.scatter(*zip(*[[x[0],x[1]] for x in grid]))\n",
    "plt.scatter(*[nplus, nminus], color='red')\n",
    "plt.plot([1,0], color='blue')\n",
    "plt.plot([0,0], color='blue')\n",
    "plt.plot((0, 0), (0, 1), 'blue')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('griddump', 'w') as f:\n",
    "    for s in grid:\n",
    "        f.write(str(s) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = pylab.figure()\n",
    "ax = Axes3D(fig)\n",
    "plt.hold(True)\n",
    "\n",
    "ax.plot([0,0], [0,1], [0,0], color = 'blue')\n",
    "ax.plot([0,1], [0,0], [0,0], color = 'blue')\n",
    "ax.plot([0,1], [1,0], [0,0], color = 'blue')\n",
    "\n",
    "x_surf=np.arange(0, 1, 0.01)                # generate a mesh\n",
    "y_surf=np.arange(0, 1, 0.01)\n",
    "x_surf, y_surf = np.meshgrid(x_surf, y_surf)\n",
    "z_surf = 0#np.sqrt(x_surf+y_surf)             # ex. function, which depends on x and y\n",
    "ax.plot_surface(x_surf, y_surf, z_surf, alpha = 0.1) \n",
    "\n",
    "XX = [x[0] for x in grid2]\n",
    "YY = [x[1] for x in grid2]\n",
    "ZZ = [x[2] for x in grid2]\n",
    "\n",
    "ax.scatter(YY, XX, ZZ)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.asarray([x[0] for x in grid])\n",
    "Y = np.asarray([x[1] for x in grid])\n",
    "Z = np.asarray([x[2] for x in grid])\n",
    "print len(X)\n",
    "#X.reshape()\n",
    "#type(X)\n",
    "#myarray = np.asarray(x)\n",
    "#type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.array([x[2] for x in grid])\n",
    "Ngrid = z.reshape((m, m)).T\n",
    "plt.imshow(Ngrid, extent=(0, 1, 0, 1), interpolation='nearest', cmap=cm.gist_rainbow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as PLT\n",
    "from matplotlib import cm as CM\n",
    "\n",
    "#A = NP.random.randint(10, 100, 100).reshape(10, 10)\n",
    "A = z\n",
    "# create an upper triangular 'matrix' from A\n",
    "A2 = z\n",
    "A2 = np.tril(A)\n",
    "fig = PLT.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "# use dir(matplotlib.cm) to get a list of the installed colormaps\n",
    "# the \"_r\" means \"reversed\" and accounts for why zero values are plotted as white\n",
    "cmap = CM.get_cmap('gray_r', 5)\n",
    "ax1.imshow(A2, interpolation=\"nearest\", cmap=cmap)\n",
    "##ax1.grid(True)\n",
    "##PLT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.random((16, 16))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## classification error\n",
    "df_cv['predictions'] = df_cv['words'].map(predict)\n",
    "df_sa_cv['predictions'] = df_sa_cv['words'].map(predict)\n",
    "df_s140_cv['predictions'] = df_s140_cv['words'].map(predict)\n",
    "\n",
    "print 'my CV: correctly classified:', 100*float(sum(df_cv['predictions'] == df_cv['sentiment']))/len(df_cv), '%'\n",
    "print 'SA CV: correctly classified:', 100*float(sum(df_sa_cv['predictions'] == df_sa_cv['sentiment']))/len(df_sa_cv), '%'\n",
    "print 'S140 CV: correctly classified:', 100*float(sum(df_s140_cv['predictions'] == df_s140_cv['sentiment']))/len(df_s140_cv), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=6\n",
    "grid = []\n",
    "for i in range(m):\n",
    "  for j in range(m):\n",
    "    if (i + j <= m -1):\n",
    "      nplus = 1.0*sum(df['sentiment']==1)/len(df)\n",
    "      nminus = 1.0*sum(df['sentiment']==-1)/len(df)\n",
    "      n0 = 1.0*sum(df['sentiment']==0)/len(df)\n",
    "      a = min(nplus,nminus,n0)\n",
    "      aplus = a*i/(nplus*(m-1))\n",
    "      aminus = a*j/(nminus*(m-1))\n",
    "      a0 = a*(1 - float(i+j)/(m-1) )/(1-nplus-nminus)\n",
    "      nplus2 = aplus*nplus/a\n",
    "      nminus2 = aminus*nminus/a\n",
    "      n02 = a0*n0/a\n",
    "      grid.append( [nplus2, nminus2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s_predict(decimate(df_sa_cv,0.3,0.2,0.5))\n",
    "print s_true(decimate(df_sa_cv,0.3,0.2,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(Aplus/A)*.14977134146341464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function that drops a fraction (1-a_i) of entries matching sentiment i\n",
    "def sent_drop_df(DF,Aplus,Aminus,A0):\n",
    "  for i in range(len(DF)):\n",
    "    if DF['sentiment'].iloc[i] == 1:\n",
    "      if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.sample(range(0, sum(df['sentiment']==1)), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## evaluate this on a grid of CV sets in (nplus, nminus) parameter space\n",
    "m=6\n",
    "grid = []\n",
    "for i in range(m):\n",
    "  for j in range(m):\n",
    "    if (i + j <= m -1):\n",
    "      nplus = 1.0*sum(df['sentiment']==1)/len(df)\n",
    "      nminus = 1.0*sum(df['sentiment']==-1)/len(df)\n",
    "      n0 = 1.0*sum(df['sentiment']==0)/len(df)\n",
    "      a = min(nplus,nminus,n0)\n",
    "      aplus = a*i/(nplus*(m-1))\n",
    "      aminus = a*j/(nminus*(m-1))\n",
    "      a0 = a*(1 - float(i+j)/(m-1) )/(1-nplus-nminus)\n",
    "      nplus2 = aplus*nplus/a\n",
    "      nminus2 = aminus*nminus/a\n",
    "      n02 = a0*n0/a\n",
    "      grid.append( [nplus2, nminus2] )\n",
    "\n",
    "## plot the grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.scatter(*zip(*grid))\n",
    "plt.scatter(*[nplus, nminus], color='red')\n",
    "plt.plot([1,0], color='blue')\n",
    "plt.plot([0,0], color='blue')\n",
    "plt.plot((0, 0), (0, 1), 'blue')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nplus, nminus, n0\n",
    "print nplus+nminus+n0\n",
    "print [aplus, aminus, a0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s_predict(df_cv)\n",
    "print s_true(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s_predict(df_sa_cv)\n",
    "print s_true(df_sa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s_predict(df_s140_cv)\n",
    "print s_true(df_s140_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2, Gavin",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
